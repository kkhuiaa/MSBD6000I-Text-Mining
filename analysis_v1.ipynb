{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import uniform, randint\n",
    "from workalendar.asia import hong_kong\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, r2_score\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Activation, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_col_list = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Open(t+1)', 'Open(t+1) >= Close', 'Volume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of pca columns: 10\n"
     ]
    }
   ],
   "source": [
    "bert_df = pd.read_csv('bert_output20171001-20200430_v1.gzip', compression='gzip', index_col='title')\n",
    "pca = PCA(n_components=.98)\n",
    "pca.fit(bert_df)\n",
    "print('number of pca columns:', pca.n_components_)\n",
    "pca_cols = ['bert_pca{}'.format(1+i) for i in range(pca.n_components_)]\n",
    "bert_pca_df = pd.DataFrame(pca.transform(bert_df), index=bert_df.index, columns=pca_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.read_csv('data/news_20171001-20200430.gzip', compression='gzip', usecols=['title', 'date']) #for mapping\n",
    "\n",
    "sentiment_df = pd.read_csv('normalised_sentiment_analysis.csv', usecols=['title_pos', 'desc_pos', 'content_pos'])\n",
    "\n",
    "sentiment_df['sum_mean'] = (sentiment_df['title_pos'] + sentiment_df['desc_pos'] + sentiment_df['content_pos'])\n",
    "\n",
    "sentiment_cols = sentiment_df.columns.tolist()\n",
    "\n",
    "news_df_merged = pd.concat([news_df, sentiment_df], axis=1)\n",
    "\n",
    "news_df_merged = news_df_merged.set_index('title').merge(bert_pca_df, left_index=True, right_index=True, how='left')\n",
    "\n",
    "news_df_merged_groupby_mean = news_df_merged.groupby('date')[pca_cols+sentiment_cols].agg(['mean'])\n",
    "\n",
    "news_df_merged_groupby_sum = news_df_merged.groupby('date')[sentiment_cols].agg(['sum'])\n",
    "\n",
    "news_df_merged_groupby = news_df_merged_groupby_mean.merge(news_df_merged_groupby_sum, left_index=True, right_index=True)\n",
    "\n",
    "news_df_merged_groupby.columns = ['_'.join(col) for col in news_df_merged_groupby.columns]\n",
    "\n",
    "news_df_merged_groupby['news_count'] = news_df_merged['date'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bert_pca1_mean</th>\n",
       "      <th>bert_pca2_mean</th>\n",
       "      <th>bert_pca3_mean</th>\n",
       "      <th>bert_pca4_mean</th>\n",
       "      <th>bert_pca5_mean</th>\n",
       "      <th>bert_pca6_mean</th>\n",
       "      <th>bert_pca7_mean</th>\n",
       "      <th>bert_pca8_mean</th>\n",
       "      <th>bert_pca9_mean</th>\n",
       "      <th>bert_pca10_mean</th>\n",
       "      <th>title_pos_mean</th>\n",
       "      <th>desc_pos_mean</th>\n",
       "      <th>content_pos_mean</th>\n",
       "      <th>sum_mean_mean</th>\n",
       "      <th>title_pos_sum</th>\n",
       "      <th>desc_pos_sum</th>\n",
       "      <th>content_pos_sum</th>\n",
       "      <th>sum_mean_sum</th>\n",
       "      <th>news_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2017-11-08</td>\n",
       "      <td>-1.133180</td>\n",
       "      <td>0.364787</td>\n",
       "      <td>0.119353</td>\n",
       "      <td>0.052549</td>\n",
       "      <td>0.143237</td>\n",
       "      <td>-0.115315</td>\n",
       "      <td>-0.140967</td>\n",
       "      <td>-0.044298</td>\n",
       "      <td>-0.100485</td>\n",
       "      <td>0.016016</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.583333</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2017-11-09</td>\n",
       "      <td>-1.503109</td>\n",
       "      <td>0.435174</td>\n",
       "      <td>0.041264</td>\n",
       "      <td>0.038034</td>\n",
       "      <td>-0.045575</td>\n",
       "      <td>-0.010922</td>\n",
       "      <td>0.011972</td>\n",
       "      <td>-0.082301</td>\n",
       "      <td>-0.102096</td>\n",
       "      <td>0.041474</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.797619</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>3.190476</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2017-11-10</td>\n",
       "      <td>-0.299249</td>\n",
       "      <td>-0.377338</td>\n",
       "      <td>-0.256372</td>\n",
       "      <td>-0.116277</td>\n",
       "      <td>-0.075168</td>\n",
       "      <td>0.144589</td>\n",
       "      <td>-0.018752</td>\n",
       "      <td>0.075183</td>\n",
       "      <td>0.019416</td>\n",
       "      <td>-0.043594</td>\n",
       "      <td>0.264172</td>\n",
       "      <td>0.278912</td>\n",
       "      <td>0.317007</td>\n",
       "      <td>0.860091</td>\n",
       "      <td>1.849206</td>\n",
       "      <td>1.952381</td>\n",
       "      <td>2.219048</td>\n",
       "      <td>6.020635</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2017-11-11</td>\n",
       "      <td>-0.666879</td>\n",
       "      <td>0.034862</td>\n",
       "      <td>0.071422</td>\n",
       "      <td>-0.071578</td>\n",
       "      <td>0.039350</td>\n",
       "      <td>0.014666</td>\n",
       "      <td>-0.079183</td>\n",
       "      <td>-0.075110</td>\n",
       "      <td>-0.084971</td>\n",
       "      <td>0.089689</td>\n",
       "      <td>0.246561</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.854894</td>\n",
       "      <td>0.986243</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>3.419577</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2017-11-12</td>\n",
       "      <td>-0.083503</td>\n",
       "      <td>-0.379441</td>\n",
       "      <td>0.014269</td>\n",
       "      <td>-0.412953</td>\n",
       "      <td>0.022082</td>\n",
       "      <td>-0.011763</td>\n",
       "      <td>-0.055076</td>\n",
       "      <td>-0.001970</td>\n",
       "      <td>-0.098344</td>\n",
       "      <td>-0.016397</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.255556</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.838889</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>1.277778</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>4.194444</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            bert_pca1_mean  bert_pca2_mean  bert_pca3_mean  bert_pca4_mean  \\\n",
       "date                                                                         \n",
       "2017-11-08       -1.133180        0.364787        0.119353        0.052549   \n",
       "2017-11-09       -1.503109        0.435174        0.041264        0.038034   \n",
       "2017-11-10       -0.299249       -0.377338       -0.256372       -0.116277   \n",
       "2017-11-11       -0.666879        0.034862        0.071422       -0.071578   \n",
       "2017-11-12       -0.083503       -0.379441        0.014269       -0.412953   \n",
       "\n",
       "            bert_pca5_mean  bert_pca6_mean  bert_pca7_mean  bert_pca8_mean  \\\n",
       "date                                                                         \n",
       "2017-11-08        0.143237       -0.115315       -0.140967       -0.044298   \n",
       "2017-11-09       -0.045575       -0.010922        0.011972       -0.082301   \n",
       "2017-11-10       -0.075168        0.144589       -0.018752        0.075183   \n",
       "2017-11-11        0.039350        0.014666       -0.079183       -0.075110   \n",
       "2017-11-12        0.022082       -0.011763       -0.055076       -0.001970   \n",
       "\n",
       "            bert_pca9_mean  bert_pca10_mean  title_pos_mean  desc_pos_mean  \\\n",
       "date                                                                         \n",
       "2017-11-08       -0.100485         0.016016        0.125000       0.333333   \n",
       "2017-11-09       -0.102096         0.041474        0.190476       0.285714   \n",
       "2017-11-10        0.019416        -0.043594        0.264172       0.278912   \n",
       "2017-11-11       -0.084971         0.089689        0.246561       0.275000   \n",
       "2017-11-12       -0.098344        -0.016397        0.250000       0.255556   \n",
       "\n",
       "            content_pos_mean  sum_mean_mean  title_pos_sum  desc_pos_sum  \\\n",
       "date                                                                       \n",
       "2017-11-08          0.333333       0.791667       0.250000      0.666667   \n",
       "2017-11-09          0.321429       0.797619       0.761905      1.142857   \n",
       "2017-11-10          0.317007       0.860091       1.849206      1.952381   \n",
       "2017-11-11          0.333333       0.854894       0.986243      1.100000   \n",
       "2017-11-12          0.333333       0.838889       1.250000      1.277778   \n",
       "\n",
       "            content_pos_sum  sum_mean_sum  news_count  \n",
       "date                                                   \n",
       "2017-11-08         0.666667      1.583333           2  \n",
       "2017-11-09         1.285714      3.190476           4  \n",
       "2017-11-10         2.219048      6.020635           7  \n",
       "2017-11-11         1.333333      3.419577           4  \n",
       "2017-11-12         1.666667      4.194444           5  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df_merged_groupby.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create holiday date columns and mapping\n",
    "\n",
    "cal = hong_kong.HongKong()\n",
    "\n",
    "holiday_list = [str(day[0]) for day in cal.holidays(2018)+cal.holidays(2019)+cal.holidays(2020)]\n",
    "\n",
    "news_df_merged_groupby['holiday'] = np.where(news_df_merged_groupby.index.isin(holiday_list), 1, 0)\n",
    "\n",
    "price_700_df = pd.read_csv('data/price_20171001-20200430.gzip', compression='gzip', index_col='Date')\n",
    "price_700_df['date'] = price_700_df.index\n",
    "\n",
    "news_df_merged_groupby2 = news_df_merged_groupby.merge(price_700_df[['date']], \n",
    "                                                       left_index=True, \n",
    "                                                       right_index=True, \n",
    "                                                       how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bert_pca1_mean</th>\n",
       "      <th>bert_pca2_mean</th>\n",
       "      <th>bert_pca3_mean</th>\n",
       "      <th>bert_pca4_mean</th>\n",
       "      <th>bert_pca5_mean</th>\n",
       "      <th>bert_pca6_mean</th>\n",
       "      <th>bert_pca7_mean</th>\n",
       "      <th>bert_pca8_mean</th>\n",
       "      <th>bert_pca9_mean</th>\n",
       "      <th>bert_pca10_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>desc_pos_mean</th>\n",
       "      <th>content_pos_mean</th>\n",
       "      <th>sum_mean_mean</th>\n",
       "      <th>title_pos_sum</th>\n",
       "      <th>desc_pos_sum</th>\n",
       "      <th>content_pos_sum</th>\n",
       "      <th>sum_mean_sum</th>\n",
       "      <th>news_count</th>\n",
       "      <th>holiday</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2017-11-08</td>\n",
       "      <td>-1.133180</td>\n",
       "      <td>0.364787</td>\n",
       "      <td>0.119353</td>\n",
       "      <td>0.052549</td>\n",
       "      <td>0.143237</td>\n",
       "      <td>-0.115315</td>\n",
       "      <td>-0.140967</td>\n",
       "      <td>-0.044298</td>\n",
       "      <td>-0.100485</td>\n",
       "      <td>0.016016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.583333</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-11-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2017-11-09</td>\n",
       "      <td>-1.503109</td>\n",
       "      <td>0.435174</td>\n",
       "      <td>0.041264</td>\n",
       "      <td>0.038034</td>\n",
       "      <td>-0.045575</td>\n",
       "      <td>-0.010922</td>\n",
       "      <td>0.011972</td>\n",
       "      <td>-0.082301</td>\n",
       "      <td>-0.102096</td>\n",
       "      <td>0.041474</td>\n",
       "      <td>...</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.797619</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>3.190476</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-11-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2017-11-10</td>\n",
       "      <td>-0.299249</td>\n",
       "      <td>-0.377338</td>\n",
       "      <td>-0.256372</td>\n",
       "      <td>-0.116277</td>\n",
       "      <td>-0.075168</td>\n",
       "      <td>0.144589</td>\n",
       "      <td>-0.018752</td>\n",
       "      <td>0.075183</td>\n",
       "      <td>0.019416</td>\n",
       "      <td>-0.043594</td>\n",
       "      <td>...</td>\n",
       "      <td>0.278912</td>\n",
       "      <td>0.317007</td>\n",
       "      <td>0.860091</td>\n",
       "      <td>1.849206</td>\n",
       "      <td>1.952381</td>\n",
       "      <td>2.219048</td>\n",
       "      <td>6.020635</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-11-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2017-11-11</td>\n",
       "      <td>-0.666879</td>\n",
       "      <td>0.034862</td>\n",
       "      <td>0.071422</td>\n",
       "      <td>-0.071578</td>\n",
       "      <td>0.039350</td>\n",
       "      <td>0.014666</td>\n",
       "      <td>-0.079183</td>\n",
       "      <td>-0.075110</td>\n",
       "      <td>-0.084971</td>\n",
       "      <td>0.089689</td>\n",
       "      <td>...</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.854894</td>\n",
       "      <td>0.986243</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>3.419577</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2017-11-12</td>\n",
       "      <td>-0.083503</td>\n",
       "      <td>-0.379441</td>\n",
       "      <td>0.014269</td>\n",
       "      <td>-0.412953</td>\n",
       "      <td>0.022082</td>\n",
       "      <td>-0.011763</td>\n",
       "      <td>-0.055076</td>\n",
       "      <td>-0.001970</td>\n",
       "      <td>-0.098344</td>\n",
       "      <td>-0.016397</td>\n",
       "      <td>...</td>\n",
       "      <td>0.255556</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.838889</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>1.277778</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>4.194444</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            bert_pca1_mean  bert_pca2_mean  bert_pca3_mean  bert_pca4_mean  \\\n",
       "date                                                                         \n",
       "2017-11-08       -1.133180        0.364787        0.119353        0.052549   \n",
       "2017-11-09       -1.503109        0.435174        0.041264        0.038034   \n",
       "2017-11-10       -0.299249       -0.377338       -0.256372       -0.116277   \n",
       "2017-11-11       -0.666879        0.034862        0.071422       -0.071578   \n",
       "2017-11-12       -0.083503       -0.379441        0.014269       -0.412953   \n",
       "\n",
       "            bert_pca5_mean  bert_pca6_mean  bert_pca7_mean  bert_pca8_mean  \\\n",
       "date                                                                         \n",
       "2017-11-08        0.143237       -0.115315       -0.140967       -0.044298   \n",
       "2017-11-09       -0.045575       -0.010922        0.011972       -0.082301   \n",
       "2017-11-10       -0.075168        0.144589       -0.018752        0.075183   \n",
       "2017-11-11        0.039350        0.014666       -0.079183       -0.075110   \n",
       "2017-11-12        0.022082       -0.011763       -0.055076       -0.001970   \n",
       "\n",
       "            bert_pca9_mean  bert_pca10_mean  ...  desc_pos_mean  \\\n",
       "date                                         ...                  \n",
       "2017-11-08       -0.100485         0.016016  ...       0.333333   \n",
       "2017-11-09       -0.102096         0.041474  ...       0.285714   \n",
       "2017-11-10        0.019416        -0.043594  ...       0.278912   \n",
       "2017-11-11       -0.084971         0.089689  ...       0.275000   \n",
       "2017-11-12       -0.098344        -0.016397  ...       0.255556   \n",
       "\n",
       "            content_pos_mean  sum_mean_mean  title_pos_sum  desc_pos_sum  \\\n",
       "date                                                                       \n",
       "2017-11-08          0.333333       0.791667       0.250000      0.666667   \n",
       "2017-11-09          0.321429       0.797619       0.761905      1.142857   \n",
       "2017-11-10          0.317007       0.860091       1.849206      1.952381   \n",
       "2017-11-11          0.333333       0.854894       0.986243      1.100000   \n",
       "2017-11-12          0.333333       0.838889       1.250000      1.277778   \n",
       "\n",
       "            content_pos_sum  sum_mean_sum  news_count  holiday        date  \n",
       "date                                                                        \n",
       "2017-11-08         0.666667      1.583333           2        0  2017-11-08  \n",
       "2017-11-09         1.285714      3.190476           4        0  2017-11-09  \n",
       "2017-11-10         2.219048      6.020635           7        0  2017-11-10  \n",
       "2017-11-11         1.333333      3.419577           4        0         NaN  \n",
       "2017-11-12         1.666667      4.194444           5        0         NaN  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df_merged_groupby2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill the Sat, Sun and holiday for mapping\n",
    "\n",
    "t = 1\n",
    "while news_df_merged_groupby2['date'].isnull().any():\n",
    "    news_df_merged_groupby2['date'] = news_df_merged_groupby2['date'].fillna(\n",
    "        news_df_merged_groupby2['date'].shift(t))\n",
    "    \n",
    "    t += 1\n",
    "    \n",
    "news_df_merged_groupby3 = news_df_merged_groupby2.groupby(news_df_merged_groupby2['date'])[\n",
    "    news_df_merged_groupby.columns.tolist()].sum()\n",
    "\n",
    "price_700_df_merged = price_700_df.merge(news_df_merged_groupby3, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>volume_adi</th>\n",
       "      <th>volume_obv</th>\n",
       "      <th>volume_cmf</th>\n",
       "      <th>volume_fi</th>\n",
       "      <th>...</th>\n",
       "      <th>title_pos_mean</th>\n",
       "      <th>desc_pos_mean</th>\n",
       "      <th>content_pos_mean</th>\n",
       "      <th>sum_mean_mean</th>\n",
       "      <th>title_pos_sum</th>\n",
       "      <th>desc_pos_sum</th>\n",
       "      <th>content_pos_sum</th>\n",
       "      <th>sum_mean_sum</th>\n",
       "      <th>news_count</th>\n",
       "      <th>holiday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2017-11-08</td>\n",
       "      <td>387.200012</td>\n",
       "      <td>398.600006</td>\n",
       "      <td>382.399994</td>\n",
       "      <td>385.600006</td>\n",
       "      <td>383.721924</td>\n",
       "      <td>43983824</td>\n",
       "      <td>5.235018e+07</td>\n",
       "      <td>88834069</td>\n",
       "      <td>0.104774</td>\n",
       "      <td>6.011343e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.583333</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2017-11-09</td>\n",
       "      <td>385.200012</td>\n",
       "      <td>392.399994</td>\n",
       "      <td>385.200012</td>\n",
       "      <td>387.799988</td>\n",
       "      <td>385.911194</td>\n",
       "      <td>23906595</td>\n",
       "      <td>4.570934e+07</td>\n",
       "      <td>112740664</td>\n",
       "      <td>0.124267</td>\n",
       "      <td>5.903923e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.797619</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>3.190476</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2017-11-10</td>\n",
       "      <td>383.399994</td>\n",
       "      <td>390.799988</td>\n",
       "      <td>383.399994</td>\n",
       "      <td>385.399994</td>\n",
       "      <td>383.522888</td>\n",
       "      <td>17152006</td>\n",
       "      <td>3.782870e+07</td>\n",
       "      <td>95588658</td>\n",
       "      <td>0.077449</td>\n",
       "      <td>4.472438e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.760733</td>\n",
       "      <td>0.809467</td>\n",
       "      <td>0.983673</td>\n",
       "      <td>2.553874</td>\n",
       "      <td>4.085450</td>\n",
       "      <td>4.330159</td>\n",
       "      <td>5.219048</td>\n",
       "      <td>13.634656</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2017-11-13</td>\n",
       "      <td>385.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>385.000000</td>\n",
       "      <td>387.600006</td>\n",
       "      <td>385.712189</td>\n",
       "      <td>17666194</td>\n",
       "      <td>3.328600e+07</td>\n",
       "      <td>113254852</td>\n",
       "      <td>0.017639</td>\n",
       "      <td>4.388745e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.264323</td>\n",
       "      <td>0.313591</td>\n",
       "      <td>0.300779</td>\n",
       "      <td>0.878693</td>\n",
       "      <td>1.850261</td>\n",
       "      <td>2.195138</td>\n",
       "      <td>2.105450</td>\n",
       "      <td>6.150849</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2017-11-14</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>387.399994</td>\n",
       "      <td>388.000000</td>\n",
       "      <td>386.110229</td>\n",
       "      <td>20492898</td>\n",
       "      <td>1.651911e+07</td>\n",
       "      <td>133747750</td>\n",
       "      <td>-0.029111</td>\n",
       "      <td>3.878882e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.264263</td>\n",
       "      <td>0.277959</td>\n",
       "      <td>0.310705</td>\n",
       "      <td>0.852927</td>\n",
       "      <td>1.321314</td>\n",
       "      <td>1.389797</td>\n",
       "      <td>1.553526</td>\n",
       "      <td>4.264637</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open        High         Low       Close   Adj Close  \\\n",
       "2017-11-08  387.200012  398.600006  382.399994  385.600006  383.721924   \n",
       "2017-11-09  385.200012  392.399994  385.200012  387.799988  385.911194   \n",
       "2017-11-10  383.399994  390.799988  383.399994  385.399994  383.522888   \n",
       "2017-11-13  385.000000  392.000000  385.000000  387.600006  385.712189   \n",
       "2017-11-14  394.000000  394.000000  387.399994  388.000000  386.110229   \n",
       "\n",
       "              Volume    volume_adi  volume_obv  volume_cmf     volume_fi  ...  \\\n",
       "2017-11-08  43983824  5.235018e+07    88834069    0.104774  6.011343e+07  ...   \n",
       "2017-11-09  23906595  4.570934e+07   112740664    0.124267  5.903923e+07  ...   \n",
       "2017-11-10  17152006  3.782870e+07    95588658    0.077449  4.472438e+07  ...   \n",
       "2017-11-13  17666194  3.328600e+07   113254852    0.017639  4.388745e+07  ...   \n",
       "2017-11-14  20492898  1.651911e+07   133747750   -0.029111  3.878882e+07  ...   \n",
       "\n",
       "            title_pos_mean  desc_pos_mean  content_pos_mean  sum_mean_mean  \\\n",
       "2017-11-08        0.125000       0.333333          0.333333       0.791667   \n",
       "2017-11-09        0.190476       0.285714          0.321429       0.797619   \n",
       "2017-11-10        0.760733       0.809467          0.983673       2.553874   \n",
       "2017-11-13        0.264323       0.313591          0.300779       0.878693   \n",
       "2017-11-14        0.264263       0.277959          0.310705       0.852927   \n",
       "\n",
       "            title_pos_sum  desc_pos_sum  content_pos_sum  sum_mean_sum  \\\n",
       "2017-11-08       0.250000      0.666667         0.666667      1.583333   \n",
       "2017-11-09       0.761905      1.142857         1.285714      3.190476   \n",
       "2017-11-10       4.085450      4.330159         5.219048     13.634656   \n",
       "2017-11-13       1.850261      2.195138         2.105450      6.150849   \n",
       "2017-11-14       1.321314      1.389797         1.553526      4.264637   \n",
       "\n",
       "            news_count  holiday  \n",
       "2017-11-08           2        0  \n",
       "2017-11-09           4        0  \n",
       "2017-11-10          16        0  \n",
       "2017-11-13           7        0  \n",
       "2017-11-14           5        0  \n",
       "\n",
       "[5 rows x 106 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_700_df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shift the columns\n",
    "\n",
    "shift_col_list = [col for col in price_700_df_merged.columns if col not in price_col_list+['date', 'holiday']]\n",
    "shift_number = 4\n",
    "\n",
    "for t in range(shift_number):\n",
    "    price_700_df_merged[[col+'(t-{})'.format(t+1) for col in shift_col_list]] = price_700_df_merged[\n",
    "        shift_col_list].shift(t+1)\n",
    "    \n",
    "price_700_df_merged = price_700_df_merged.iloc[shift_number:, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>volume_adi</th>\n",
       "      <th>volume_obv</th>\n",
       "      <th>volume_cmf</th>\n",
       "      <th>volume_fi</th>\n",
       "      <th>...</th>\n",
       "      <th>bert_pca10_mean(t-4)</th>\n",
       "      <th>title_pos_mean(t-4)</th>\n",
       "      <th>desc_pos_mean(t-4)</th>\n",
       "      <th>content_pos_mean(t-4)</th>\n",
       "      <th>sum_mean_mean(t-4)</th>\n",
       "      <th>title_pos_sum(t-4)</th>\n",
       "      <th>desc_pos_sum(t-4)</th>\n",
       "      <th>content_pos_sum(t-4)</th>\n",
       "      <th>sum_mean_sum(t-4)</th>\n",
       "      <th>news_count(t-4)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2017-11-14</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>387.399994</td>\n",
       "      <td>388.000000</td>\n",
       "      <td>386.110229</td>\n",
       "      <td>20492898</td>\n",
       "      <td>1.651911e+07</td>\n",
       "      <td>133747750</td>\n",
       "      <td>-0.029111</td>\n",
       "      <td>3.878882e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016016</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.583333</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2017-11-15</td>\n",
       "      <td>383.799988</td>\n",
       "      <td>388.000000</td>\n",
       "      <td>383.000000</td>\n",
       "      <td>383.000000</td>\n",
       "      <td>381.134583</td>\n",
       "      <td>23436345</td>\n",
       "      <td>-6.917232e+06</td>\n",
       "      <td>110311405</td>\n",
       "      <td>-0.116871</td>\n",
       "      <td>1.650731e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041474</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.797619</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>3.190476</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2017-11-16</td>\n",
       "      <td>388.200012</td>\n",
       "      <td>393.799988</td>\n",
       "      <td>384.000000</td>\n",
       "      <td>391.799988</td>\n",
       "      <td>389.891693</td>\n",
       "      <td>32583706</td>\n",
       "      <td>1.236699e+07</td>\n",
       "      <td>142895111</td>\n",
       "      <td>-0.017458</td>\n",
       "      <td>5.511144e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029698</td>\n",
       "      <td>0.760733</td>\n",
       "      <td>0.809467</td>\n",
       "      <td>0.983673</td>\n",
       "      <td>2.553874</td>\n",
       "      <td>4.085450</td>\n",
       "      <td>4.330159</td>\n",
       "      <td>5.219048</td>\n",
       "      <td>13.634656</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2017-11-17</td>\n",
       "      <td>397.000000</td>\n",
       "      <td>405.000000</td>\n",
       "      <td>397.000000</td>\n",
       "      <td>403.399994</td>\n",
       "      <td>401.435242</td>\n",
       "      <td>34542285</td>\n",
       "      <td>3.309230e+07</td>\n",
       "      <td>177437396</td>\n",
       "      <td>0.024822</td>\n",
       "      <td>1.044799e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164980</td>\n",
       "      <td>0.264323</td>\n",
       "      <td>0.313591</td>\n",
       "      <td>0.300779</td>\n",
       "      <td>0.878693</td>\n",
       "      <td>1.850261</td>\n",
       "      <td>2.195138</td>\n",
       "      <td>2.105450</td>\n",
       "      <td>6.150849</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2017-11-20</td>\n",
       "      <td>405.399994</td>\n",
       "      <td>420.000000</td>\n",
       "      <td>405.399994</td>\n",
       "      <td>420.000000</td>\n",
       "      <td>417.954376</td>\n",
       "      <td>35231740</td>\n",
       "      <td>6.832404e+07</td>\n",
       "      <td>212669136</td>\n",
       "      <td>0.111815</td>\n",
       "      <td>1.731038e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025316</td>\n",
       "      <td>0.264263</td>\n",
       "      <td>0.277959</td>\n",
       "      <td>0.310705</td>\n",
       "      <td>0.852927</td>\n",
       "      <td>1.321314</td>\n",
       "      <td>1.389797</td>\n",
       "      <td>1.553526</td>\n",
       "      <td>4.264637</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 490 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open        High         Low       Close   Adj Close  \\\n",
       "2017-11-14  394.000000  394.000000  387.399994  388.000000  386.110229   \n",
       "2017-11-15  383.799988  388.000000  383.000000  383.000000  381.134583   \n",
       "2017-11-16  388.200012  393.799988  384.000000  391.799988  389.891693   \n",
       "2017-11-17  397.000000  405.000000  397.000000  403.399994  401.435242   \n",
       "2017-11-20  405.399994  420.000000  405.399994  420.000000  417.954376   \n",
       "\n",
       "              Volume    volume_adi  volume_obv  volume_cmf     volume_fi  ...  \\\n",
       "2017-11-14  20492898  1.651911e+07   133747750   -0.029111  3.878882e+07  ...   \n",
       "2017-11-15  23436345 -6.917232e+06   110311405   -0.116871  1.650731e+07  ...   \n",
       "2017-11-16  32583706  1.236699e+07   142895111   -0.017458  5.511144e+07  ...   \n",
       "2017-11-17  34542285  3.309230e+07   177437396    0.024822  1.044799e+08  ...   \n",
       "2017-11-20  35231740  6.832404e+07   212669136    0.111815  1.731038e+08  ...   \n",
       "\n",
       "            bert_pca10_mean(t-4)  title_pos_mean(t-4)  desc_pos_mean(t-4)  \\\n",
       "2017-11-14              0.016016             0.125000            0.333333   \n",
       "2017-11-15              0.041474             0.190476            0.285714   \n",
       "2017-11-16              0.029698             0.760733            0.809467   \n",
       "2017-11-17              0.164980             0.264323            0.313591   \n",
       "2017-11-20              0.025316             0.264263            0.277959   \n",
       "\n",
       "            content_pos_mean(t-4)  sum_mean_mean(t-4)  title_pos_sum(t-4)  \\\n",
       "2017-11-14               0.333333            0.791667            0.250000   \n",
       "2017-11-15               0.321429            0.797619            0.761905   \n",
       "2017-11-16               0.983673            2.553874            4.085450   \n",
       "2017-11-17               0.300779            0.878693            1.850261   \n",
       "2017-11-20               0.310705            0.852927            1.321314   \n",
       "\n",
       "            desc_pos_sum(t-4)  content_pos_sum(t-4)  sum_mean_sum(t-4)  \\\n",
       "2017-11-14           0.666667              0.666667           1.583333   \n",
       "2017-11-15           1.142857              1.285714           3.190476   \n",
       "2017-11-16           4.330159              5.219048          13.634656   \n",
       "2017-11-17           2.195138              2.105450           6.150849   \n",
       "2017-11-20           1.389797              1.553526           4.264637   \n",
       "\n",
       "            news_count(t-4)  \n",
       "2017-11-14              2.0  \n",
       "2017-11-15              4.0  \n",
       "2017-11-16             16.0  \n",
       "2017-11-17              7.0  \n",
       "2017-11-20              5.0  \n",
       "\n",
       "[5 rows x 490 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_700_df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create date related columns\n",
    "\n",
    "price_700_df['date'] = pd.to_datetime(price_700_df.index, yearfirst=True)\n",
    "price_700_df['month'] = price_700_df['date'].dt.month\n",
    "price_700_df['day'] = price_700_df['date'].dt.day\n",
    "price_700_df['weekday'] = price_700_df['date'].dt.weekday\n",
    "# price_700_df_merged.head()\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(price_700_df_merged.drop(price_col_list+['date'], axis=1)\n",
    "    # , price_700_df_merged['Open(t+1) >= Close'], test_size=.2, random_state=1, stratify=price_700_df_merged['Open(t+1) >= Close'])\n",
    "\n",
    "data_len = price_700_df_merged.shape[0]\n",
    "X_train = price_700_df_merged.drop(price_col_list+['date'], axis=1).iloc[:9*data_len//10:, :]\n",
    "X_test = price_700_df_merged.drop(price_col_list+['date'], axis=1).iloc[9*data_len//10:, :]\n",
    "y_train = price_700_df_merged['Open(t+1) >= Close'].iloc[:9*data_len//10]\n",
    "y_test = price_700_df_merged['Open(t+1) >= Close'].iloc[9*data_len//10:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(544, 481)\n",
      "target rate 0.6213235294117647\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "\n",
    "for col in X_train:\n",
    "    if X_train[col].isnull().sum() > 0:\n",
    "        print('missing is found in:', col)\n",
    "print('target rate', y_train.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    mean_train_score  mean_test_score\n",
      "1           0.848219         0.751495\n",
      "19          0.856203         0.761597\n",
      "8           0.856251         0.767037\n",
      "18          0.860741         0.768293\n",
      "16          0.861586         0.777499\n",
      "4           0.862147         0.777618\n",
      "6           0.858264         0.779173\n",
      "17          0.879267         0.783596\n",
      "12          0.784837         0.787243\n",
      "7           0.852959         0.788977\n",
      "10          0.786248         0.790471\n",
      "11          0.782204         0.790710\n",
      "14          0.800509         0.790890\n",
      "3           0.787860         0.791786\n",
      "13          0.860105         0.793161\n",
      "2           0.799933         0.793639\n",
      "5           0.796409         0.793998\n",
      "0           0.801811         0.794835\n",
      "9           0.790007         0.795433\n",
      "15          0.791796         0.797226\n",
      "                            col  importance\n",
      "75           sp500_index_change    0.289444\n",
      "74          nasdaq_index_change    0.254685\n",
      "76             dji_index_change    0.234991\n",
      "371         bert_pca6_mean(t-3)    0.123989\n",
      "337         trend_kst_diff(t-3)    0.096890\n",
      "322         trend_sma_slow(t-3)    0.000000\n",
      "319      trend_macd_signal(t-3)    0.000000\n",
      "320        trend_macd_diff(t-3)    0.000000\n",
      "321         trend_sma_fast(t-3)    0.000000\n",
      "323         trend_ema_fast(t-3)    0.000000\n",
      "317        volatility_dcli(t-3)    0.000000\n",
      "324         trend_ema_slow(t-3)    0.000000\n",
      "325              trend_adx(t-3)    0.000000\n",
      "326          trend_adx_pos(t-3)    0.000000\n",
      "327          trend_adx_neg(t-3)    0.000000\n",
      "328   trend_vortex_ind_pos(t-3)    0.000000\n",
      "318             trend_macd(t-3)    0.000000\n",
      "0                    volume_adi    0.000000\n",
      "316        volatility_dchi(t-3)    0.000000\n",
      "330  trend_vortex_ind_diff(t-3)    0.000000\n",
      "315         volatility_dch(t-3)    0.000000\n",
      "314         volatility_dcl(t-3)    0.000000\n",
      "313        volatility_kcli(t-3)    0.000000\n",
      "312        volatility_kchi(t-3)    0.000000\n",
      "311         volatility_kcp(t-3)    0.000000\n",
      "310         volatility_kcw(t-3)    0.000000\n",
      "309         volatility_kcl(t-3)    0.000000\n",
      "308         volatility_kch(t-3)    0.000000\n",
      "307         volatility_kcc(t-3)    0.000000\n",
      "306        volatility_bbli(t-3)    0.000000\n",
      "roc_auc in train: 0.7938401217900846\n",
      "roc_auc in test: 0.6137387387387387\n"
     ]
    }
   ],
   "source": [
    "random_state_model = 1\n",
    "n_jobs = 15\n",
    "\n",
    "xgbc = XGBClassifier(random_state=random_state_model, n_jobs=n_jobs, eval_set=[(X_train, y_train)], eval_metric='auc', early_stopping_rounds=5)\n",
    "\n",
    "param_dist = {'n_estimator': randint(10, 15), \n",
    "              'max_depth': randint(1, 3), \n",
    "              'learning_rate': uniform(.005, .01), \n",
    "              'min_child_weight': uniform(.01, .1),\n",
    "              'subsample': uniform(.3, .6), \n",
    "              'lambda': uniform(2, 3), #l2\n",
    "              'alpha': uniform(1, 3)} #l1\n",
    "\n",
    "rs_cv = RandomizedSearchCV(xgbc, param_dist, n_iter=20, return_train_score=True, n_jobs=n_jobs, scoring='roc_auc',\n",
    "                           cv=StratifiedShuffleSplit(n_splits=3, test_size=.2, random_state=random_state_model))\n",
    "\n",
    "rs_cv.fit(X_train, y_train)\n",
    "\n",
    "cv_result = pd.DataFrame.from_dict(rs_cv.cv_results_).sort_values('rank_test_score', ascending=False)\n",
    "print(cv_result[['mean_train_score', 'mean_test_score']])\n",
    "\n",
    "importance_feature_df = pd.DataFrame({'col': X_train.columns, \n",
    "                                      'importance': rs_cv.best_estimator_.feature_importances_})\n",
    "\n",
    "print(importance_feature_df.sort_values('importance', ascending=False).head(30))\n",
    "\n",
    "print('roc_auc in train:', roc_auc_score(y_train, rs_cv.best_estimator_.predict_proba(X_train)[:,  1]))\n",
    "print('roc_auc in test:', roc_auc_score(y_test, rs_cv.best_estimator_.predict_proba(X_test)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X lstm train shape (535, 10, 103)\n",
      "y lstm train shape (535,)\n",
      "Train on 535 samples, validate on 60 samples\n",
      "Epoch 1/500\n",
      " - 1s - loss: 0.6934 - accuracy: 0.5645 - val_loss: 0.6521 - val_accuracy: 0.6167\n",
      "Epoch 2/500\n",
      " - 0s - loss: 0.6877 - accuracy: 0.6019 - val_loss: 0.6593 - val_accuracy: 0.6167\n",
      "Epoch 3/500\n",
      " - 0s - loss: 0.6735 - accuracy: 0.6150 - val_loss: 0.6557 - val_accuracy: 0.6167\n",
      "Epoch 4/500\n",
      " - 0s - loss: 0.6674 - accuracy: 0.6131 - val_loss: 0.6531 - val_accuracy: 0.6167\n",
      "Epoch 5/500\n",
      " - 0s - loss: 0.6696 - accuracy: 0.6150 - val_loss: 0.6531 - val_accuracy: 0.6167\n",
      "Epoch 6/500\n",
      " - 0s - loss: 0.6723 - accuracy: 0.6187 - val_loss: 0.6539 - val_accuracy: 0.6167\n",
      "Epoch 7/500\n",
      " - 0s - loss: 0.6591 - accuracy: 0.6168 - val_loss: 0.6533 - val_accuracy: 0.6167\n",
      "Epoch 8/500\n",
      " - 0s - loss: 0.6649 - accuracy: 0.6224 - val_loss: 0.6534 - val_accuracy: 0.6167\n",
      "Epoch 9/500\n",
      " - 0s - loss: 0.6669 - accuracy: 0.6187 - val_loss: 0.6526 - val_accuracy: 0.6167\n",
      "Epoch 10/500\n",
      " - 0s - loss: 0.6665 - accuracy: 0.6187 - val_loss: 0.6521 - val_accuracy: 0.6167\n",
      "Epoch 11/500\n",
      " - 0s - loss: 0.6631 - accuracy: 0.6243 - val_loss: 0.6517 - val_accuracy: 0.6167\n",
      "Epoch 12/500\n",
      " - 0s - loss: 0.6648 - accuracy: 0.6187 - val_loss: 0.6502 - val_accuracy: 0.6167\n",
      "Epoch 13/500\n",
      " - 0s - loss: 0.6621 - accuracy: 0.6224 - val_loss: 0.6501 - val_accuracy: 0.6167\n",
      "Epoch 14/500\n",
      " - 0s - loss: 0.6603 - accuracy: 0.6206 - val_loss: 0.6488 - val_accuracy: 0.6167\n",
      "Epoch 15/500\n",
      " - 0s - loss: 0.6622 - accuracy: 0.6224 - val_loss: 0.6481 - val_accuracy: 0.6167\n",
      "Epoch 16/500\n",
      " - 0s - loss: 0.6636 - accuracy: 0.6168 - val_loss: 0.6470 - val_accuracy: 0.6167\n",
      "Epoch 17/500\n",
      " - 0s - loss: 0.6650 - accuracy: 0.6224 - val_loss: 0.6477 - val_accuracy: 0.6167\n",
      "Epoch 18/500\n",
      " - 0s - loss: 0.6674 - accuracy: 0.6168 - val_loss: 0.6497 - val_accuracy: 0.6167\n",
      "Epoch 19/500\n",
      " - 0s - loss: 0.6576 - accuracy: 0.6224 - val_loss: 0.6505 - val_accuracy: 0.6167\n",
      "Epoch 20/500\n",
      " - 0s - loss: 0.6594 - accuracy: 0.6150 - val_loss: 0.6512 - val_accuracy: 0.6167\n",
      "Epoch 21/500\n",
      " - 0s - loss: 0.6597 - accuracy: 0.6206 - val_loss: 0.6501 - val_accuracy: 0.6167\n",
      "Epoch 22/500\n",
      " - 0s - loss: 0.6612 - accuracy: 0.6187 - val_loss: 0.6494 - val_accuracy: 0.6167\n",
      "Epoch 23/500\n",
      " - 0s - loss: 0.6647 - accuracy: 0.6150 - val_loss: 0.6508 - val_accuracy: 0.6167\n",
      "Epoch 24/500\n",
      " - 0s - loss: 0.6573 - accuracy: 0.6187 - val_loss: 0.6489 - val_accuracy: 0.6167\n",
      "Epoch 25/500\n",
      " - 0s - loss: 0.6614 - accuracy: 0.6206 - val_loss: 0.6522 - val_accuracy: 0.6167\n",
      "Epoch 26/500\n",
      " - 0s - loss: 0.6619 - accuracy: 0.6187 - val_loss: 0.6512 - val_accuracy: 0.6167\n",
      "Epoch 27/500\n",
      " - 0s - loss: 0.6620 - accuracy: 0.6187 - val_loss: 0.6512 - val_accuracy: 0.6167\n",
      "Epoch 28/500\n",
      " - 0s - loss: 0.6619 - accuracy: 0.6206 - val_loss: 0.6519 - val_accuracy: 0.6167\n",
      "Epoch 29/500\n",
      " - 0s - loss: 0.6618 - accuracy: 0.6187 - val_loss: 0.6525 - val_accuracy: 0.6167\n",
      "Epoch 30/500\n",
      " - 0s - loss: 0.6554 - accuracy: 0.6262 - val_loss: 0.6522 - val_accuracy: 0.6167\n",
      "Epoch 31/500\n",
      " - 0s - loss: 0.6612 - accuracy: 0.6206 - val_loss: 0.6528 - val_accuracy: 0.6167\n",
      "Epoch 32/500\n",
      " - 0s - loss: 0.6638 - accuracy: 0.6206 - val_loss: 0.6539 - val_accuracy: 0.6167\n",
      "Epoch 33/500\n",
      " - 0s - loss: 0.6550 - accuracy: 0.6206 - val_loss: 0.6527 - val_accuracy: 0.6167\n",
      "Epoch 34/500\n",
      " - 0s - loss: 0.6585 - accuracy: 0.6168 - val_loss: 0.6517 - val_accuracy: 0.6167\n",
      "Epoch 35/500\n",
      " - 0s - loss: 0.6592 - accuracy: 0.6168 - val_loss: 0.6518 - val_accuracy: 0.6167\n",
      "Epoch 36/500\n",
      " - 0s - loss: 0.6599 - accuracy: 0.6243 - val_loss: 0.6514 - val_accuracy: 0.6167\n",
      "Epoch 37/500\n",
      " - 0s - loss: 0.6636 - accuracy: 0.6187 - val_loss: 0.6503 - val_accuracy: 0.6167\n",
      "Epoch 38/500\n",
      " - 0s - loss: 0.6623 - accuracy: 0.6206 - val_loss: 0.6502 - val_accuracy: 0.6167\n",
      "Epoch 39/500\n",
      " - 0s - loss: 0.6653 - accuracy: 0.6224 - val_loss: 0.6487 - val_accuracy: 0.6167\n",
      "Epoch 40/500\n",
      " - 0s - loss: 0.6541 - accuracy: 0.6187 - val_loss: 0.6504 - val_accuracy: 0.6167\n",
      "Epoch 41/500\n",
      " - 0s - loss: 0.6636 - accuracy: 0.6206 - val_loss: 0.6499 - val_accuracy: 0.6167\n",
      "Epoch 42/500\n",
      " - 0s - loss: 0.6614 - accuracy: 0.6206 - val_loss: 0.6502 - val_accuracy: 0.6167\n",
      "Epoch 43/500\n",
      " - 0s - loss: 0.6599 - accuracy: 0.6224 - val_loss: 0.6504 - val_accuracy: 0.6167\n",
      "Epoch 44/500\n",
      " - 0s - loss: 0.6594 - accuracy: 0.6224 - val_loss: 0.6496 - val_accuracy: 0.6167\n",
      "Epoch 45/500\n",
      " - 0s - loss: 0.6602 - accuracy: 0.6206 - val_loss: 0.6496 - val_accuracy: 0.6167\n",
      "Epoch 46/500\n",
      " - 0s - loss: 0.6615 - accuracy: 0.6187 - val_loss: 0.6486 - val_accuracy: 0.6167\n",
      "Epoch 47/500\n",
      " - 0s - loss: 0.6580 - accuracy: 0.6187 - val_loss: 0.6507 - val_accuracy: 0.6167\n",
      "Epoch 48/500\n",
      " - 0s - loss: 0.6604 - accuracy: 0.6206 - val_loss: 0.6523 - val_accuracy: 0.6167\n",
      "Epoch 49/500\n",
      " - 0s - loss: 0.6628 - accuracy: 0.6206 - val_loss: 0.6537 - val_accuracy: 0.6167\n",
      "Epoch 50/500\n",
      " - 0s - loss: 0.6609 - accuracy: 0.6206 - val_loss: 0.6536 - val_accuracy: 0.6167\n",
      "Epoch 51/500\n",
      " - 0s - loss: 0.6615 - accuracy: 0.6224 - val_loss: 0.6524 - val_accuracy: 0.6167\n",
      "Epoch 52/500\n",
      " - 0s - loss: 0.6547 - accuracy: 0.6243 - val_loss: 0.6524 - val_accuracy: 0.6167\n",
      "Epoch 53/500\n",
      " - 0s - loss: 0.6622 - accuracy: 0.6243 - val_loss: 0.6532 - val_accuracy: 0.6167\n",
      "Epoch 54/500\n",
      " - 0s - loss: 0.6616 - accuracy: 0.6206 - val_loss: 0.6529 - val_accuracy: 0.6167\n",
      "Epoch 55/500\n",
      " - 0s - loss: 0.6612 - accuracy: 0.6243 - val_loss: 0.6519 - val_accuracy: 0.6167\n",
      "Epoch 56/500\n",
      " - 0s - loss: 0.6565 - accuracy: 0.6243 - val_loss: 0.6519 - val_accuracy: 0.6167\n",
      "Epoch 57/500\n",
      " - 0s - loss: 0.6557 - accuracy: 0.6168 - val_loss: 0.6528 - val_accuracy: 0.6167\n",
      "Epoch 58/500\n",
      " - 0s - loss: 0.6588 - accuracy: 0.6224 - val_loss: 0.6532 - val_accuracy: 0.6167\n",
      "Epoch 59/500\n",
      " - 0s - loss: 0.6608 - accuracy: 0.6187 - val_loss: 0.6505 - val_accuracy: 0.6167\n",
      "Epoch 60/500\n",
      " - 0s - loss: 0.6591 - accuracy: 0.6224 - val_loss: 0.6522 - val_accuracy: 0.6167\n",
      "Epoch 61/500\n",
      " - 0s - loss: 0.6523 - accuracy: 0.6224 - val_loss: 0.6519 - val_accuracy: 0.6167\n",
      "Epoch 62/500\n",
      " - 0s - loss: 0.6607 - accuracy: 0.6224 - val_loss: 0.6528 - val_accuracy: 0.6167\n",
      "Epoch 63/500\n",
      " - 0s - loss: 0.6568 - accuracy: 0.6187 - val_loss: 0.6515 - val_accuracy: 0.6167\n",
      "Epoch 64/500\n",
      " - 0s - loss: 0.6561 - accuracy: 0.6206 - val_loss: 0.6510 - val_accuracy: 0.6167\n",
      "Epoch 65/500\n",
      " - 0s - loss: 0.6629 - accuracy: 0.6262 - val_loss: 0.6488 - val_accuracy: 0.6167\n",
      "Epoch 66/500\n",
      " - 0s - loss: 0.6605 - accuracy: 0.6150 - val_loss: 0.6505 - val_accuracy: 0.6167\n",
      "Epoch 67/500\n",
      " - 0s - loss: 0.6584 - accuracy: 0.6206 - val_loss: 0.6513 - val_accuracy: 0.6167\n",
      "Epoch 68/500\n",
      " - 0s - loss: 0.6621 - accuracy: 0.6168 - val_loss: 0.6497 - val_accuracy: 0.6167\n",
      "Epoch 69/500\n",
      " - 0s - loss: 0.6523 - accuracy: 0.6224 - val_loss: 0.6491 - val_accuracy: 0.6167\n",
      "Epoch 70/500\n",
      " - 0s - loss: 0.6664 - accuracy: 0.6206 - val_loss: 0.6489 - val_accuracy: 0.6167\n",
      "Epoch 71/500\n",
      " - 0s - loss: 0.6566 - accuracy: 0.6187 - val_loss: 0.6490 - val_accuracy: 0.6167\n",
      "Epoch 72/500\n",
      " - 0s - loss: 0.6612 - accuracy: 0.6187 - val_loss: 0.6493 - val_accuracy: 0.6167\n",
      "Epoch 73/500\n",
      " - 0s - loss: 0.6615 - accuracy: 0.6224 - val_loss: 0.6487 - val_accuracy: 0.6167\n",
      "Epoch 74/500\n",
      " - 0s - loss: 0.6560 - accuracy: 0.6224 - val_loss: 0.6499 - val_accuracy: 0.6167\n",
      "Epoch 75/500\n",
      " - 0s - loss: 0.6588 - accuracy: 0.6150 - val_loss: 0.6505 - val_accuracy: 0.6167\n",
      "Epoch 76/500\n",
      " - 0s - loss: 0.6575 - accuracy: 0.6187 - val_loss: 0.6507 - val_accuracy: 0.6167\n",
      "Epoch 77/500\n",
      " - 0s - loss: 0.6571 - accuracy: 0.6206 - val_loss: 0.6498 - val_accuracy: 0.6167\n",
      "Epoch 78/500\n",
      " - 0s - loss: 0.6591 - accuracy: 0.6206 - val_loss: 0.6505 - val_accuracy: 0.6167\n",
      "Epoch 79/500\n",
      " - 0s - loss: 0.6563 - accuracy: 0.6206 - val_loss: 0.6514 - val_accuracy: 0.6167\n",
      "Epoch 80/500\n",
      " - 0s - loss: 0.6543 - accuracy: 0.6206 - val_loss: 0.6501 - val_accuracy: 0.6167\n",
      "Epoch 81/500\n",
      " - 0s - loss: 0.6583 - accuracy: 0.6243 - val_loss: 0.6486 - val_accuracy: 0.6167\n",
      "Epoch 82/500\n",
      " - 0s - loss: 0.6587 - accuracy: 0.6206 - val_loss: 0.6493 - val_accuracy: 0.6167\n",
      "Epoch 83/500\n",
      " - 0s - loss: 0.6587 - accuracy: 0.6206 - val_loss: 0.6499 - val_accuracy: 0.6167\n",
      "Epoch 84/500\n",
      " - 0s - loss: 0.6515 - accuracy: 0.6243 - val_loss: 0.6495 - val_accuracy: 0.6167\n",
      "Epoch 85/500\n",
      " - 0s - loss: 0.6566 - accuracy: 0.6150 - val_loss: 0.6483 - val_accuracy: 0.6167\n",
      "Epoch 86/500\n",
      " - 0s - loss: 0.6601 - accuracy: 0.6206 - val_loss: 0.6480 - val_accuracy: 0.6167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/500\n",
      " - 0s - loss: 0.6560 - accuracy: 0.6150 - val_loss: 0.6473 - val_accuracy: 0.6167\n",
      "Epoch 88/500\n",
      " - 0s - loss: 0.6590 - accuracy: 0.6187 - val_loss: 0.6482 - val_accuracy: 0.6167\n",
      "Epoch 89/500\n",
      " - 0s - loss: 0.6563 - accuracy: 0.6243 - val_loss: 0.6478 - val_accuracy: 0.6167\n",
      "Epoch 90/500\n",
      " - 0s - loss: 0.6597 - accuracy: 0.6187 - val_loss: 0.6493 - val_accuracy: 0.6167\n",
      "Epoch 91/500\n",
      " - 0s - loss: 0.6617 - accuracy: 0.6168 - val_loss: 0.6491 - val_accuracy: 0.6167\n",
      "Epoch 92/500\n",
      " - 0s - loss: 0.6547 - accuracy: 0.6243 - val_loss: 0.6493 - val_accuracy: 0.6167\n",
      "Epoch 93/500\n",
      " - 0s - loss: 0.6569 - accuracy: 0.6262 - val_loss: 0.6507 - val_accuracy: 0.6167\n",
      "Epoch 94/500\n",
      " - 0s - loss: 0.6553 - accuracy: 0.6150 - val_loss: 0.6512 - val_accuracy: 0.6167\n",
      "Epoch 95/500\n",
      " - 0s - loss: 0.6639 - accuracy: 0.6150 - val_loss: 0.6514 - val_accuracy: 0.6167\n",
      "Epoch 96/500\n",
      " - 0s - loss: 0.6514 - accuracy: 0.6206 - val_loss: 0.6513 - val_accuracy: 0.6167\n",
      "Epoch 97/500\n",
      " - 0s - loss: 0.6514 - accuracy: 0.6206 - val_loss: 0.6500 - val_accuracy: 0.6167\n",
      "Epoch 98/500\n",
      " - 0s - loss: 0.6522 - accuracy: 0.6187 - val_loss: 0.6491 - val_accuracy: 0.6167\n",
      "Epoch 99/500\n",
      " - 0s - loss: 0.6561 - accuracy: 0.6206 - val_loss: 0.6481 - val_accuracy: 0.6167\n",
      "Epoch 100/500\n",
      " - 0s - loss: 0.6528 - accuracy: 0.6206 - val_loss: 0.6505 - val_accuracy: 0.6167\n",
      "Epoch 101/500\n",
      " - 0s - loss: 0.6661 - accuracy: 0.6112 - val_loss: 0.6504 - val_accuracy: 0.6167\n",
      "Epoch 102/500\n",
      " - 0s - loss: 0.6560 - accuracy: 0.6243 - val_loss: 0.6495 - val_accuracy: 0.6167\n",
      "Epoch 103/500\n",
      " - 0s - loss: 0.6557 - accuracy: 0.6262 - val_loss: 0.6481 - val_accuracy: 0.6167\n",
      "Epoch 104/500\n",
      " - 0s - loss: 0.6581 - accuracy: 0.6187 - val_loss: 0.6480 - val_accuracy: 0.6167\n",
      "Epoch 105/500\n",
      " - 0s - loss: 0.6581 - accuracy: 0.6280 - val_loss: 0.6494 - val_accuracy: 0.6167\n",
      "Epoch 106/500\n",
      " - 0s - loss: 0.6619 - accuracy: 0.6112 - val_loss: 0.6494 - val_accuracy: 0.6167\n",
      "Epoch 107/500\n",
      " - 0s - loss: 0.6548 - accuracy: 0.6224 - val_loss: 0.6488 - val_accuracy: 0.6167\n",
      "Epoch 108/500\n",
      " - 0s - loss: 0.6644 - accuracy: 0.6187 - val_loss: 0.6468 - val_accuracy: 0.6167\n",
      "Epoch 109/500\n",
      " - 0s - loss: 0.6614 - accuracy: 0.6150 - val_loss: 0.6467 - val_accuracy: 0.6167\n",
      "Epoch 110/500\n",
      " - 0s - loss: 0.6596 - accuracy: 0.6206 - val_loss: 0.6468 - val_accuracy: 0.6167\n",
      "Epoch 111/500\n",
      " - 0s - loss: 0.6537 - accuracy: 0.6262 - val_loss: 0.6451 - val_accuracy: 0.6167\n",
      "Epoch 112/500\n",
      " - 0s - loss: 0.6579 - accuracy: 0.6224 - val_loss: 0.6456 - val_accuracy: 0.6167\n",
      "Epoch 113/500\n",
      " - 0s - loss: 0.6590 - accuracy: 0.6187 - val_loss: 0.6463 - val_accuracy: 0.6167\n",
      "Epoch 114/500\n",
      " - 0s - loss: 0.6530 - accuracy: 0.6243 - val_loss: 0.6462 - val_accuracy: 0.6167\n",
      "Epoch 115/500\n",
      " - 0s - loss: 0.6559 - accuracy: 0.6206 - val_loss: 0.6469 - val_accuracy: 0.6167\n",
      "Epoch 116/500\n",
      " - 0s - loss: 0.6587 - accuracy: 0.6056 - val_loss: 0.6485 - val_accuracy: 0.6167\n",
      "Epoch 117/500\n",
      " - 0s - loss: 0.6537 - accuracy: 0.6243 - val_loss: 0.6503 - val_accuracy: 0.6167\n",
      "Epoch 118/500\n",
      " - 0s - loss: 0.6558 - accuracy: 0.6187 - val_loss: 0.6505 - val_accuracy: 0.6167\n",
      "Epoch 119/500\n",
      " - 0s - loss: 0.6590 - accuracy: 0.6206 - val_loss: 0.6513 - val_accuracy: 0.6167\n",
      "Epoch 120/500\n",
      " - 0s - loss: 0.6605 - accuracy: 0.6206 - val_loss: 0.6516 - val_accuracy: 0.6167\n",
      "Epoch 121/500\n",
      " - 0s - loss: 0.6573 - accuracy: 0.6243 - val_loss: 0.6520 - val_accuracy: 0.6167\n",
      "Epoch 122/500\n",
      " - 0s - loss: 0.6615 - accuracy: 0.6150 - val_loss: 0.6515 - val_accuracy: 0.6167\n",
      "Epoch 123/500\n",
      " - 0s - loss: 0.6545 - accuracy: 0.6299 - val_loss: 0.6523 - val_accuracy: 0.6167\n",
      "Epoch 124/500\n",
      " - 0s - loss: 0.6589 - accuracy: 0.6206 - val_loss: 0.6515 - val_accuracy: 0.6167\n",
      "Epoch 125/500\n",
      " - 0s - loss: 0.6492 - accuracy: 0.6262 - val_loss: 0.6513 - val_accuracy: 0.6167\n",
      "Epoch 126/500\n",
      " - 0s - loss: 0.6558 - accuracy: 0.6280 - val_loss: 0.6512 - val_accuracy: 0.6167\n",
      "Epoch 127/500\n",
      " - 0s - loss: 0.6555 - accuracy: 0.6224 - val_loss: 0.6513 - val_accuracy: 0.6167\n",
      "Epoch 128/500\n",
      " - 0s - loss: 0.6574 - accuracy: 0.6280 - val_loss: 0.6501 - val_accuracy: 0.6167\n",
      "Epoch 129/500\n",
      " - 0s - loss: 0.6654 - accuracy: 0.6131 - val_loss: 0.6495 - val_accuracy: 0.6167\n",
      "Epoch 130/500\n",
      " - 0s - loss: 0.6620 - accuracy: 0.6280 - val_loss: 0.6484 - val_accuracy: 0.6167\n",
      "Epoch 131/500\n",
      " - 0s - loss: 0.6559 - accuracy: 0.6262 - val_loss: 0.6485 - val_accuracy: 0.6167\n",
      "Epoch 132/500\n",
      " - 0s - loss: 0.6577 - accuracy: 0.6206 - val_loss: 0.6492 - val_accuracy: 0.6167\n",
      "Epoch 133/500\n",
      " - 0s - loss: 0.6596 - accuracy: 0.6150 - val_loss: 0.6499 - val_accuracy: 0.6167\n",
      "Epoch 134/500\n",
      " - 0s - loss: 0.6512 - accuracy: 0.6168 - val_loss: 0.6482 - val_accuracy: 0.6167\n",
      "Epoch 135/500\n",
      " - 0s - loss: 0.6567 - accuracy: 0.6168 - val_loss: 0.6492 - val_accuracy: 0.6167\n",
      "Epoch 136/500\n",
      " - 0s - loss: 0.6590 - accuracy: 0.6187 - val_loss: 0.6472 - val_accuracy: 0.6167\n",
      "Epoch 137/500\n",
      " - 0s - loss: 0.6614 - accuracy: 0.6224 - val_loss: 0.6479 - val_accuracy: 0.6167\n",
      "Epoch 138/500\n",
      " - 0s - loss: 0.6529 - accuracy: 0.6150 - val_loss: 0.6478 - val_accuracy: 0.6167\n",
      "Epoch 139/500\n",
      " - 0s - loss: 0.6565 - accuracy: 0.6262 - val_loss: 0.6516 - val_accuracy: 0.6167\n",
      "Epoch 140/500\n",
      " - 0s - loss: 0.6607 - accuracy: 0.6224 - val_loss: 0.6506 - val_accuracy: 0.6167\n",
      "Epoch 141/500\n",
      " - 0s - loss: 0.6496 - accuracy: 0.6206 - val_loss: 0.6502 - val_accuracy: 0.6167\n",
      "Epoch 142/500\n",
      " - 0s - loss: 0.6605 - accuracy: 0.6224 - val_loss: 0.6516 - val_accuracy: 0.6167\n",
      "Epoch 143/500\n",
      " - 0s - loss: 0.6591 - accuracy: 0.6168 - val_loss: 0.6530 - val_accuracy: 0.6167\n",
      "Epoch 144/500\n",
      " - 0s - loss: 0.6576 - accuracy: 0.6168 - val_loss: 0.6524 - val_accuracy: 0.6167\n",
      "Epoch 145/500\n",
      " - 0s - loss: 0.6570 - accuracy: 0.6243 - val_loss: 0.6498 - val_accuracy: 0.6167\n",
      "Epoch 146/500\n",
      " - 0s - loss: 0.6541 - accuracy: 0.6224 - val_loss: 0.6503 - val_accuracy: 0.6167\n",
      "Epoch 147/500\n",
      " - 0s - loss: 0.6528 - accuracy: 0.6224 - val_loss: 0.6473 - val_accuracy: 0.6167\n",
      "Epoch 148/500\n",
      " - 0s - loss: 0.6569 - accuracy: 0.6150 - val_loss: 0.6455 - val_accuracy: 0.6333\n",
      "Epoch 149/500\n",
      " - 0s - loss: 0.6563 - accuracy: 0.6187 - val_loss: 0.6459 - val_accuracy: 0.6167\n",
      "Epoch 150/500\n",
      " - 0s - loss: 0.6578 - accuracy: 0.6224 - val_loss: 0.6462 - val_accuracy: 0.6167\n",
      "Epoch 151/500\n",
      " - 0s - loss: 0.6563 - accuracy: 0.6224 - val_loss: 0.6457 - val_accuracy: 0.6167\n",
      "Epoch 152/500\n",
      " - 0s - loss: 0.6601 - accuracy: 0.6224 - val_loss: 0.6453 - val_accuracy: 0.6167\n",
      "Epoch 153/500\n",
      " - 0s - loss: 0.6545 - accuracy: 0.6150 - val_loss: 0.6448 - val_accuracy: 0.6167\n",
      "Epoch 154/500\n",
      " - 0s - loss: 0.6652 - accuracy: 0.6206 - val_loss: 0.6453 - val_accuracy: 0.6167\n",
      "Epoch 155/500\n",
      " - 0s - loss: 0.6526 - accuracy: 0.6262 - val_loss: 0.6459 - val_accuracy: 0.6167\n",
      "Epoch 156/500\n",
      " - 0s - loss: 0.6544 - accuracy: 0.6280 - val_loss: 0.6468 - val_accuracy: 0.6167\n",
      "Epoch 157/500\n",
      " - 0s - loss: 0.6542 - accuracy: 0.6224 - val_loss: 0.6466 - val_accuracy: 0.6333\n",
      "Epoch 158/500\n",
      " - 0s - loss: 0.6552 - accuracy: 0.6280 - val_loss: 0.6459 - val_accuracy: 0.6167\n",
      "Epoch 159/500\n",
      " - 0s - loss: 0.6506 - accuracy: 0.6243 - val_loss: 0.6426 - val_accuracy: 0.6333\n",
      "Epoch 160/500\n",
      " - 0s - loss: 0.6567 - accuracy: 0.6224 - val_loss: 0.6454 - val_accuracy: 0.6167\n",
      "Epoch 161/500\n",
      " - 0s - loss: 0.6479 - accuracy: 0.6262 - val_loss: 0.6455 - val_accuracy: 0.6333\n",
      "Epoch 162/500\n",
      " - 0s - loss: 0.6600 - accuracy: 0.6224 - val_loss: 0.6446 - val_accuracy: 0.6167\n",
      "Epoch 163/500\n",
      " - 0s - loss: 0.6506 - accuracy: 0.6243 - val_loss: 0.6455 - val_accuracy: 0.6167\n",
      "Epoch 164/500\n",
      " - 0s - loss: 0.6505 - accuracy: 0.6280 - val_loss: 0.6449 - val_accuracy: 0.6167\n",
      "Epoch 165/500\n",
      " - 0s - loss: 0.6505 - accuracy: 0.6243 - val_loss: 0.6447 - val_accuracy: 0.6167\n",
      "Epoch 166/500\n",
      " - 0s - loss: 0.6591 - accuracy: 0.6168 - val_loss: 0.6436 - val_accuracy: 0.6167\n",
      "Epoch 167/500\n",
      " - 0s - loss: 0.6529 - accuracy: 0.6243 - val_loss: 0.6463 - val_accuracy: 0.6333\n",
      "Epoch 168/500\n",
      " - 0s - loss: 0.6516 - accuracy: 0.6206 - val_loss: 0.6446 - val_accuracy: 0.6333\n",
      "Epoch 169/500\n",
      " - 0s - loss: 0.6491 - accuracy: 0.6224 - val_loss: 0.6476 - val_accuracy: 0.6167\n",
      "Epoch 170/500\n",
      " - 0s - loss: 0.6541 - accuracy: 0.6206 - val_loss: 0.6438 - val_accuracy: 0.6333\n",
      "Epoch 171/500\n",
      " - 0s - loss: 0.6470 - accuracy: 0.6336 - val_loss: 0.6430 - val_accuracy: 0.6167\n",
      "Epoch 172/500\n",
      " - 0s - loss: 0.6561 - accuracy: 0.6187 - val_loss: 0.6437 - val_accuracy: 0.6333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 173/500\n",
      " - 0s - loss: 0.6575 - accuracy: 0.6262 - val_loss: 0.6442 - val_accuracy: 0.6167\n",
      "Epoch 174/500\n",
      " - 0s - loss: 0.6547 - accuracy: 0.6262 - val_loss: 0.6480 - val_accuracy: 0.6167\n",
      "Epoch 175/500\n",
      " - 0s - loss: 0.6519 - accuracy: 0.6187 - val_loss: 0.6497 - val_accuracy: 0.6167\n",
      "Epoch 176/500\n",
      " - 0s - loss: 0.6500 - accuracy: 0.6280 - val_loss: 0.6507 - val_accuracy: 0.6167\n",
      "Epoch 177/500\n",
      " - 0s - loss: 0.6526 - accuracy: 0.6243 - val_loss: 0.6531 - val_accuracy: 0.6167\n",
      "Epoch 178/500\n",
      " - 0s - loss: 0.6472 - accuracy: 0.6336 - val_loss: 0.6502 - val_accuracy: 0.6167\n",
      "Epoch 179/500\n",
      " - 0s - loss: 0.6516 - accuracy: 0.6411 - val_loss: 0.6470 - val_accuracy: 0.6167\n",
      "Epoch 180/500\n",
      " - 0s - loss: 0.6552 - accuracy: 0.6280 - val_loss: 0.6493 - val_accuracy: 0.6167\n",
      "Epoch 181/500\n",
      " - 0s - loss: 0.6534 - accuracy: 0.6280 - val_loss: 0.6482 - val_accuracy: 0.6167\n",
      "Epoch 182/500\n",
      " - 0s - loss: 0.6544 - accuracy: 0.6150 - val_loss: 0.6470 - val_accuracy: 0.6167\n",
      "Epoch 183/500\n",
      " - 0s - loss: 0.6496 - accuracy: 0.6262 - val_loss: 0.6498 - val_accuracy: 0.6167\n",
      "Epoch 184/500\n",
      " - 0s - loss: 0.6483 - accuracy: 0.6411 - val_loss: 0.6503 - val_accuracy: 0.6167\n",
      "Epoch 185/500\n",
      " - 0s - loss: 0.6565 - accuracy: 0.6262 - val_loss: 0.6496 - val_accuracy: 0.6167\n",
      "Epoch 186/500\n",
      " - 0s - loss: 0.6469 - accuracy: 0.6336 - val_loss: 0.6485 - val_accuracy: 0.6167\n",
      "Epoch 187/500\n",
      " - 0s - loss: 0.6493 - accuracy: 0.6187 - val_loss: 0.6486 - val_accuracy: 0.6167\n",
      "Epoch 188/500\n",
      " - 0s - loss: 0.6601 - accuracy: 0.6131 - val_loss: 0.6470 - val_accuracy: 0.6167\n",
      "Epoch 189/500\n",
      " - 0s - loss: 0.6641 - accuracy: 0.6168 - val_loss: 0.6489 - val_accuracy: 0.6333\n",
      "Epoch 190/500\n",
      " - 0s - loss: 0.6534 - accuracy: 0.6299 - val_loss: 0.6478 - val_accuracy: 0.6167\n",
      "Epoch 191/500\n",
      " - 0s - loss: 0.6558 - accuracy: 0.6187 - val_loss: 0.6483 - val_accuracy: 0.6167\n",
      "Epoch 192/500\n",
      " - 0s - loss: 0.6569 - accuracy: 0.6168 - val_loss: 0.6474 - val_accuracy: 0.6167\n",
      "Epoch 193/500\n",
      " - 0s - loss: 0.6523 - accuracy: 0.6374 - val_loss: 0.6480 - val_accuracy: 0.6167\n",
      "Epoch 194/500\n",
      " - 0s - loss: 0.6526 - accuracy: 0.6168 - val_loss: 0.6465 - val_accuracy: 0.6167\n",
      "Epoch 195/500\n",
      " - 0s - loss: 0.6512 - accuracy: 0.6243 - val_loss: 0.6432 - val_accuracy: 0.6167\n",
      "Epoch 196/500\n",
      " - 0s - loss: 0.6593 - accuracy: 0.6206 - val_loss: 0.6424 - val_accuracy: 0.6167\n",
      "Epoch 197/500\n",
      " - 0s - loss: 0.6519 - accuracy: 0.6280 - val_loss: 0.6435 - val_accuracy: 0.6167\n",
      "Epoch 198/500\n",
      " - 0s - loss: 0.6488 - accuracy: 0.6393 - val_loss: 0.6450 - val_accuracy: 0.6167\n",
      "Epoch 199/500\n",
      " - 0s - loss: 0.6528 - accuracy: 0.6262 - val_loss: 0.6485 - val_accuracy: 0.6167\n",
      "Epoch 200/500\n",
      " - 0s - loss: 0.6464 - accuracy: 0.6374 - val_loss: 0.6496 - val_accuracy: 0.6167\n",
      "Epoch 201/500\n",
      " - 0s - loss: 0.6554 - accuracy: 0.6355 - val_loss: 0.6501 - val_accuracy: 0.6167\n",
      "Epoch 202/500\n",
      " - 0s - loss: 0.6573 - accuracy: 0.6243 - val_loss: 0.6503 - val_accuracy: 0.6167\n",
      "Epoch 203/500\n",
      " - 0s - loss: 0.6494 - accuracy: 0.6299 - val_loss: 0.6494 - val_accuracy: 0.6167\n",
      "Epoch 204/500\n",
      " - 0s - loss: 0.6504 - accuracy: 0.6112 - val_loss: 0.6500 - val_accuracy: 0.6167\n",
      "Epoch 205/500\n",
      " - 0s - loss: 0.6484 - accuracy: 0.6206 - val_loss: 0.6515 - val_accuracy: 0.6167\n",
      "Epoch 206/500\n",
      " - 0s - loss: 0.6484 - accuracy: 0.6224 - val_loss: 0.6513 - val_accuracy: 0.6167\n",
      "Epoch 207/500\n",
      " - 0s - loss: 0.6464 - accuracy: 0.6280 - val_loss: 0.6477 - val_accuracy: 0.6167\n",
      "Epoch 208/500\n",
      " - 0s - loss: 0.6619 - accuracy: 0.6243 - val_loss: 0.6460 - val_accuracy: 0.6167\n",
      "Epoch 209/500\n",
      " - 0s - loss: 0.6548 - accuracy: 0.6262 - val_loss: 0.6502 - val_accuracy: 0.6167\n",
      "Epoch 210/500\n",
      " - 0s - loss: 0.6495 - accuracy: 0.6224 - val_loss: 0.6522 - val_accuracy: 0.6167\n",
      "Epoch 211/500\n",
      " - 0s - loss: 0.6551 - accuracy: 0.6299 - val_loss: 0.6531 - val_accuracy: 0.6167\n",
      "Epoch 212/500\n",
      " - 0s - loss: 0.6541 - accuracy: 0.6299 - val_loss: 0.6481 - val_accuracy: 0.6167\n",
      "Epoch 213/500\n",
      " - 0s - loss: 0.6515 - accuracy: 0.6374 - val_loss: 0.6473 - val_accuracy: 0.6167\n",
      "Epoch 214/500\n",
      " - 0s - loss: 0.6549 - accuracy: 0.6168 - val_loss: 0.6497 - val_accuracy: 0.6167\n",
      "Epoch 215/500\n",
      " - 0s - loss: 0.6509 - accuracy: 0.6131 - val_loss: 0.6495 - val_accuracy: 0.6167\n",
      "Epoch 216/500\n",
      " - 0s - loss: 0.6528 - accuracy: 0.6262 - val_loss: 0.6482 - val_accuracy: 0.6167\n",
      "Epoch 217/500\n",
      " - 0s - loss: 0.6517 - accuracy: 0.6243 - val_loss: 0.6494 - val_accuracy: 0.6167\n",
      "Epoch 218/500\n",
      " - 0s - loss: 0.6497 - accuracy: 0.6280 - val_loss: 0.6477 - val_accuracy: 0.6167\n",
      "Epoch 219/500\n",
      " - 0s - loss: 0.6490 - accuracy: 0.6168 - val_loss: 0.6436 - val_accuracy: 0.6167\n",
      "Epoch 220/500\n",
      " - 0s - loss: 0.6517 - accuracy: 0.6262 - val_loss: 0.6451 - val_accuracy: 0.6167\n",
      "Epoch 221/500\n",
      " - 0s - loss: 0.6548 - accuracy: 0.6318 - val_loss: 0.6482 - val_accuracy: 0.6167\n",
      "Epoch 222/500\n",
      " - 0s - loss: 0.6491 - accuracy: 0.6262 - val_loss: 0.6489 - val_accuracy: 0.6167\n",
      "Epoch 223/500\n",
      " - 0s - loss: 0.6553 - accuracy: 0.6206 - val_loss: 0.6452 - val_accuracy: 0.6167\n",
      "Epoch 224/500\n",
      " - 0s - loss: 0.6573 - accuracy: 0.6206 - val_loss: 0.6448 - val_accuracy: 0.6167\n",
      "Epoch 225/500\n",
      " - 0s - loss: 0.6559 - accuracy: 0.6150 - val_loss: 0.6433 - val_accuracy: 0.6167\n",
      "Epoch 226/500\n",
      " - 0s - loss: 0.6503 - accuracy: 0.6336 - val_loss: 0.6428 - val_accuracy: 0.6167\n",
      "Epoch 227/500\n",
      " - 0s - loss: 0.6507 - accuracy: 0.6355 - val_loss: 0.6432 - val_accuracy: 0.6167\n",
      "Epoch 228/500\n",
      " - 0s - loss: 0.6507 - accuracy: 0.6224 - val_loss: 0.6504 - val_accuracy: 0.6167\n",
      "Epoch 229/500\n",
      " - 0s - loss: 0.6500 - accuracy: 0.6374 - val_loss: 0.6592 - val_accuracy: 0.6167\n",
      "Epoch 230/500\n",
      " - 0s - loss: 0.6524 - accuracy: 0.6411 - val_loss: 0.6576 - val_accuracy: 0.6333\n",
      "Epoch 231/500\n",
      " - 0s - loss: 0.6571 - accuracy: 0.6206 - val_loss: 0.6500 - val_accuracy: 0.6333\n",
      "Epoch 232/500\n",
      " - 0s - loss: 0.6531 - accuracy: 0.6224 - val_loss: 0.6445 - val_accuracy: 0.6333\n",
      "Epoch 233/500\n",
      " - 0s - loss: 0.6443 - accuracy: 0.6430 - val_loss: 0.6434 - val_accuracy: 0.6167\n",
      "Epoch 234/500\n",
      " - 0s - loss: 0.6549 - accuracy: 0.6374 - val_loss: 0.6490 - val_accuracy: 0.6333\n",
      "Epoch 235/500\n",
      " - 0s - loss: 0.6556 - accuracy: 0.6336 - val_loss: 0.6512 - val_accuracy: 0.6167\n",
      "Epoch 236/500\n",
      " - 0s - loss: 0.6482 - accuracy: 0.6206 - val_loss: 0.6500 - val_accuracy: 0.6167\n",
      "Epoch 237/500\n",
      " - 0s - loss: 0.6559 - accuracy: 0.6318 - val_loss: 0.6476 - val_accuracy: 0.6167\n",
      "Epoch 238/500\n",
      " - 0s - loss: 0.6489 - accuracy: 0.6299 - val_loss: 0.6468 - val_accuracy: 0.6167\n",
      "Epoch 239/500\n",
      " - 0s - loss: 0.6525 - accuracy: 0.6355 - val_loss: 0.6478 - val_accuracy: 0.6167\n",
      "Epoch 240/500\n",
      " - 0s - loss: 0.6553 - accuracy: 0.6262 - val_loss: 0.6499 - val_accuracy: 0.6167\n",
      "Epoch 241/500\n",
      " - 0s - loss: 0.6564 - accuracy: 0.6187 - val_loss: 0.6473 - val_accuracy: 0.6167\n",
      "Epoch 242/500\n",
      " - 0s - loss: 0.6430 - accuracy: 0.6486 - val_loss: 0.6474 - val_accuracy: 0.6167\n",
      "Epoch 243/500\n",
      " - 0s - loss: 0.6567 - accuracy: 0.6280 - val_loss: 0.6457 - val_accuracy: 0.6167\n",
      "Epoch 244/500\n",
      " - 0s - loss: 0.6540 - accuracy: 0.6336 - val_loss: 0.6461 - val_accuracy: 0.6167\n",
      "Epoch 245/500\n",
      " - 0s - loss: 0.6573 - accuracy: 0.6112 - val_loss: 0.6421 - val_accuracy: 0.6167\n",
      "Epoch 246/500\n",
      " - 0s - loss: 0.6455 - accuracy: 0.6243 - val_loss: 0.6426 - val_accuracy: 0.6167\n",
      "Epoch 247/500\n",
      " - 0s - loss: 0.6473 - accuracy: 0.6243 - val_loss: 0.6401 - val_accuracy: 0.6167\n",
      "Epoch 248/500\n",
      " - 0s - loss: 0.6470 - accuracy: 0.6224 - val_loss: 0.6421 - val_accuracy: 0.6167\n",
      "Epoch 249/500\n",
      " - 0s - loss: 0.6560 - accuracy: 0.6280 - val_loss: 0.6463 - val_accuracy: 0.6000\n",
      "Epoch 250/500\n",
      " - 0s - loss: 0.6445 - accuracy: 0.6299 - val_loss: 0.6471 - val_accuracy: 0.6167\n",
      "Epoch 251/500\n",
      " - 0s - loss: 0.6596 - accuracy: 0.6224 - val_loss: 0.6467 - val_accuracy: 0.6167\n",
      "Epoch 252/500\n",
      " - 0s - loss: 0.6481 - accuracy: 0.6336 - val_loss: 0.6466 - val_accuracy: 0.6167\n",
      "Epoch 253/500\n",
      " - 0s - loss: 0.6489 - accuracy: 0.6318 - val_loss: 0.6488 - val_accuracy: 0.6167\n",
      "Epoch 254/500\n",
      " - 0s - loss: 0.6567 - accuracy: 0.6150 - val_loss: 0.6491 - val_accuracy: 0.6167\n",
      "Epoch 255/500\n",
      " - 0s - loss: 0.6595 - accuracy: 0.6206 - val_loss: 0.6496 - val_accuracy: 0.6167\n",
      "Epoch 256/500\n",
      " - 0s - loss: 0.6539 - accuracy: 0.6280 - val_loss: 0.6507 - val_accuracy: 0.6167\n",
      "Epoch 257/500\n",
      " - 0s - loss: 0.6567 - accuracy: 0.6262 - val_loss: 0.6497 - val_accuracy: 0.6167\n",
      "Epoch 258/500\n",
      " - 0s - loss: 0.6446 - accuracy: 0.6112 - val_loss: 0.6501 - val_accuracy: 0.6167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 259/500\n",
      " - 0s - loss: 0.6455 - accuracy: 0.6224 - val_loss: 0.6498 - val_accuracy: 0.6167\n",
      "Epoch 260/500\n",
      " - 0s - loss: 0.6487 - accuracy: 0.6318 - val_loss: 0.6507 - val_accuracy: 0.6167\n",
      "Epoch 261/500\n",
      " - 0s - loss: 0.6456 - accuracy: 0.6318 - val_loss: 0.6508 - val_accuracy: 0.6167\n",
      "Epoch 262/500\n",
      " - 0s - loss: 0.6457 - accuracy: 0.6336 - val_loss: 0.6510 - val_accuracy: 0.6167\n",
      "Epoch 263/500\n",
      " - 0s - loss: 0.6516 - accuracy: 0.6150 - val_loss: 0.6493 - val_accuracy: 0.6167\n",
      "Epoch 264/500\n",
      " - 0s - loss: 0.6453 - accuracy: 0.6262 - val_loss: 0.6485 - val_accuracy: 0.6167\n",
      "Epoch 265/500\n",
      " - 0s - loss: 0.6523 - accuracy: 0.6150 - val_loss: 0.6484 - val_accuracy: 0.6167\n",
      "Epoch 266/500\n",
      " - 0s - loss: 0.6526 - accuracy: 0.6224 - val_loss: 0.6472 - val_accuracy: 0.6167\n",
      "Epoch 267/500\n",
      " - 0s - loss: 0.6538 - accuracy: 0.6206 - val_loss: 0.6450 - val_accuracy: 0.6167\n",
      "Epoch 268/500\n",
      " - 0s - loss: 0.6538 - accuracy: 0.6168 - val_loss: 0.6462 - val_accuracy: 0.5667\n",
      "Epoch 269/500\n",
      " - 0s - loss: 0.6541 - accuracy: 0.6224 - val_loss: 0.6506 - val_accuracy: 0.6167\n",
      "Epoch 270/500\n",
      " - 0s - loss: 0.6501 - accuracy: 0.6318 - val_loss: 0.6506 - val_accuracy: 0.6167\n",
      "Epoch 271/500\n",
      " - 0s - loss: 0.6535 - accuracy: 0.6224 - val_loss: 0.6485 - val_accuracy: 0.6167\n",
      "Epoch 272/500\n",
      " - 0s - loss: 0.6574 - accuracy: 0.6262 - val_loss: 0.6472 - val_accuracy: 0.6167\n",
      "Epoch 273/500\n",
      " - 0s - loss: 0.6469 - accuracy: 0.6224 - val_loss: 0.6434 - val_accuracy: 0.6167\n",
      "Epoch 274/500\n",
      " - 0s - loss: 0.6557 - accuracy: 0.6280 - val_loss: 0.6428 - val_accuracy: 0.6167\n",
      "Epoch 275/500\n",
      " - 0s - loss: 0.6446 - accuracy: 0.6318 - val_loss: 0.6398 - val_accuracy: 0.5833\n",
      "Epoch 276/500\n",
      " - 0s - loss: 0.6551 - accuracy: 0.6206 - val_loss: 0.6400 - val_accuracy: 0.5833\n",
      "Epoch 277/500\n",
      " - 0s - loss: 0.6388 - accuracy: 0.6430 - val_loss: 0.6420 - val_accuracy: 0.6167\n",
      "Epoch 278/500\n",
      " - 0s - loss: 0.6574 - accuracy: 0.6224 - val_loss: 0.6413 - val_accuracy: 0.6167\n",
      "Epoch 279/500\n",
      " - 0s - loss: 0.6536 - accuracy: 0.6131 - val_loss: 0.6453 - val_accuracy: 0.6167\n",
      "Epoch 280/500\n",
      " - 0s - loss: 0.6384 - accuracy: 0.6579 - val_loss: 0.6469 - val_accuracy: 0.6167\n",
      "Epoch 281/500\n",
      " - 0s - loss: 0.6492 - accuracy: 0.6318 - val_loss: 0.6480 - val_accuracy: 0.6167\n",
      "Epoch 282/500\n",
      " - 0s - loss: 0.6543 - accuracy: 0.6206 - val_loss: 0.6537 - val_accuracy: 0.6167\n",
      "Epoch 283/500\n",
      " - 0s - loss: 0.6485 - accuracy: 0.6150 - val_loss: 0.6511 - val_accuracy: 0.6167\n",
      "Epoch 284/500\n",
      " - 0s - loss: 0.6534 - accuracy: 0.6336 - val_loss: 0.6483 - val_accuracy: 0.6000\n",
      "Epoch 285/500\n",
      " - 0s - loss: 0.6462 - accuracy: 0.6206 - val_loss: 0.6526 - val_accuracy: 0.6167\n",
      "Epoch 286/500\n",
      " - 0s - loss: 0.6514 - accuracy: 0.6336 - val_loss: 0.6536 - val_accuracy: 0.6167\n",
      "Epoch 287/500\n",
      " - 0s - loss: 0.6423 - accuracy: 0.6430 - val_loss: 0.6483 - val_accuracy: 0.6167\n",
      "Epoch 288/500\n",
      " - 0s - loss: 0.6479 - accuracy: 0.6150 - val_loss: 0.6457 - val_accuracy: 0.6167\n",
      "Epoch 289/500\n",
      " - 0s - loss: 0.6383 - accuracy: 0.6393 - val_loss: 0.6459 - val_accuracy: 0.6167\n",
      "Epoch 290/500\n",
      " - 0s - loss: 0.6496 - accuracy: 0.6374 - val_loss: 0.6460 - val_accuracy: 0.6167\n",
      "Epoch 291/500\n",
      " - 0s - loss: 0.6500 - accuracy: 0.6393 - val_loss: 0.6463 - val_accuracy: 0.6000\n",
      "Epoch 292/500\n",
      " - 0s - loss: 0.6596 - accuracy: 0.6243 - val_loss: 0.6449 - val_accuracy: 0.6167\n",
      "Epoch 293/500\n",
      " - 0s - loss: 0.6480 - accuracy: 0.6355 - val_loss: 0.6408 - val_accuracy: 0.6167\n",
      "Epoch 294/500\n",
      " - 0s - loss: 0.6489 - accuracy: 0.6449 - val_loss: 0.6378 - val_accuracy: 0.6167\n",
      "Epoch 295/500\n",
      " - 0s - loss: 0.6495 - accuracy: 0.6150 - val_loss: 0.6385 - val_accuracy: 0.6167\n",
      "Epoch 296/500\n",
      " - 0s - loss: 0.6482 - accuracy: 0.6318 - val_loss: 0.6408 - val_accuracy: 0.5833\n",
      "Epoch 297/500\n",
      " - 0s - loss: 0.6467 - accuracy: 0.6430 - val_loss: 0.6436 - val_accuracy: 0.6167\n",
      "Epoch 298/500\n",
      " - 0s - loss: 0.6534 - accuracy: 0.6224 - val_loss: 0.6448 - val_accuracy: 0.6167\n",
      "Epoch 299/500\n",
      " - 0s - loss: 0.6424 - accuracy: 0.6449 - val_loss: 0.6449 - val_accuracy: 0.5833\n",
      "Epoch 300/500\n",
      " - 0s - loss: 0.6479 - accuracy: 0.6355 - val_loss: 0.6449 - val_accuracy: 0.6167\n",
      "Epoch 301/500\n",
      " - 0s - loss: 0.6533 - accuracy: 0.6243 - val_loss: 0.6467 - val_accuracy: 0.6167\n",
      "Epoch 302/500\n",
      " - 0s - loss: 0.6450 - accuracy: 0.6299 - val_loss: 0.6478 - val_accuracy: 0.6167\n",
      "Epoch 303/500\n",
      " - 0s - loss: 0.6540 - accuracy: 0.6355 - val_loss: 0.6491 - val_accuracy: 0.6000\n",
      "Epoch 304/500\n",
      " - 0s - loss: 0.6407 - accuracy: 0.6411 - val_loss: 0.6516 - val_accuracy: 0.6167\n",
      "Epoch 305/500\n",
      " - 0s - loss: 0.6453 - accuracy: 0.6299 - val_loss: 0.6473 - val_accuracy: 0.6000\n",
      "Epoch 306/500\n",
      " - 0s - loss: 0.6569 - accuracy: 0.6206 - val_loss: 0.6482 - val_accuracy: 0.6167\n",
      "Epoch 307/500\n",
      " - 0s - loss: 0.6408 - accuracy: 0.6411 - val_loss: 0.6470 - val_accuracy: 0.6000\n",
      "Epoch 308/500\n",
      " - 0s - loss: 0.6560 - accuracy: 0.6318 - val_loss: 0.6431 - val_accuracy: 0.6167\n",
      "Epoch 309/500\n",
      " - 0s - loss: 0.6506 - accuracy: 0.6318 - val_loss: 0.6430 - val_accuracy: 0.6000\n",
      "Epoch 310/500\n",
      " - 0s - loss: 0.6458 - accuracy: 0.6374 - val_loss: 0.6474 - val_accuracy: 0.6167\n",
      "Epoch 311/500\n",
      " - 0s - loss: 0.6509 - accuracy: 0.6299 - val_loss: 0.6526 - val_accuracy: 0.6167\n",
      "Epoch 312/500\n",
      " - 0s - loss: 0.6467 - accuracy: 0.6374 - val_loss: 0.6594 - val_accuracy: 0.6167\n",
      "Epoch 313/500\n",
      " - 0s - loss: 0.6488 - accuracy: 0.6224 - val_loss: 0.6621 - val_accuracy: 0.6167\n",
      "Epoch 314/500\n",
      " - 0s - loss: 0.6597 - accuracy: 0.6075 - val_loss: 0.6594 - val_accuracy: 0.6167\n",
      "Epoch 315/500\n",
      " - 0s - loss: 0.6448 - accuracy: 0.6336 - val_loss: 0.6590 - val_accuracy: 0.6167\n",
      "Epoch 316/500\n",
      " - 0s - loss: 0.6549 - accuracy: 0.6262 - val_loss: 0.6571 - val_accuracy: 0.6167\n",
      "Epoch 317/500\n",
      " - 0s - loss: 0.6374 - accuracy: 0.6374 - val_loss: 0.6558 - val_accuracy: 0.6167\n",
      "Epoch 318/500\n",
      " - 0s - loss: 0.6514 - accuracy: 0.6243 - val_loss: 0.6546 - val_accuracy: 0.6167\n",
      "Epoch 319/500\n",
      " - 0s - loss: 0.6456 - accuracy: 0.6355 - val_loss: 0.6524 - val_accuracy: 0.6167\n",
      "Epoch 320/500\n",
      " - 0s - loss: 0.6438 - accuracy: 0.6336 - val_loss: 0.6538 - val_accuracy: 0.6167\n",
      "Epoch 321/500\n",
      " - 0s - loss: 0.6537 - accuracy: 0.6280 - val_loss: 0.6552 - val_accuracy: 0.6167\n",
      "Epoch 322/500\n",
      " - 0s - loss: 0.6437 - accuracy: 0.6299 - val_loss: 0.6573 - val_accuracy: 0.6167\n",
      "Epoch 323/500\n",
      " - 0s - loss: 0.6449 - accuracy: 0.6336 - val_loss: 0.6575 - val_accuracy: 0.6167\n",
      "Epoch 324/500\n",
      " - 0s - loss: 0.6480 - accuracy: 0.6243 - val_loss: 0.6574 - val_accuracy: 0.6167\n",
      "Epoch 325/500\n",
      " - 0s - loss: 0.6415 - accuracy: 0.6393 - val_loss: 0.6533 - val_accuracy: 0.6167\n",
      "Epoch 326/500\n",
      " - 0s - loss: 0.6489 - accuracy: 0.6411 - val_loss: 0.6521 - val_accuracy: 0.6167\n",
      "Epoch 327/500\n",
      " - 0s - loss: 0.6412 - accuracy: 0.6411 - val_loss: 0.6520 - val_accuracy: 0.6167\n",
      "Epoch 328/500\n",
      " - 0s - loss: 0.6454 - accuracy: 0.6355 - val_loss: 0.6503 - val_accuracy: 0.6167\n",
      "Epoch 329/500\n",
      " - 0s - loss: 0.6379 - accuracy: 0.6430 - val_loss: 0.6456 - val_accuracy: 0.6167\n",
      "Epoch 330/500\n",
      " - 0s - loss: 0.6483 - accuracy: 0.6131 - val_loss: 0.6462 - val_accuracy: 0.6000\n",
      "Epoch 331/500\n",
      " - 0s - loss: 0.6468 - accuracy: 0.6374 - val_loss: 0.6473 - val_accuracy: 0.6167\n",
      "Epoch 332/500\n",
      " - 0s - loss: 0.6467 - accuracy: 0.6262 - val_loss: 0.6487 - val_accuracy: 0.6000\n",
      "Epoch 333/500\n",
      " - 0s - loss: 0.6501 - accuracy: 0.6355 - val_loss: 0.6496 - val_accuracy: 0.5667\n",
      "Epoch 334/500\n",
      " - 0s - loss: 0.6369 - accuracy: 0.6411 - val_loss: 0.6516 - val_accuracy: 0.6167\n",
      "Epoch 335/500\n",
      " - 0s - loss: 0.6502 - accuracy: 0.6187 - val_loss: 0.6497 - val_accuracy: 0.5667\n",
      "Epoch 336/500\n",
      " - 0s - loss: 0.6414 - accuracy: 0.6355 - val_loss: 0.6483 - val_accuracy: 0.6000\n",
      "Epoch 337/500\n",
      " - 0s - loss: 0.6387 - accuracy: 0.6411 - val_loss: 0.6495 - val_accuracy: 0.5833\n",
      "Epoch 338/500\n",
      " - 0s - loss: 0.6466 - accuracy: 0.6243 - val_loss: 0.6482 - val_accuracy: 0.5833\n",
      "Epoch 339/500\n",
      " - 0s - loss: 0.6472 - accuracy: 0.6299 - val_loss: 0.6481 - val_accuracy: 0.6000\n",
      "Epoch 340/500\n",
      " - 0s - loss: 0.6437 - accuracy: 0.6467 - val_loss: 0.6500 - val_accuracy: 0.6000\n",
      "Epoch 341/500\n",
      " - 0s - loss: 0.6519 - accuracy: 0.6112 - val_loss: 0.6502 - val_accuracy: 0.6000\n",
      "Epoch 342/500\n",
      " - 0s - loss: 0.6493 - accuracy: 0.6168 - val_loss: 0.6481 - val_accuracy: 0.6167\n",
      "Epoch 343/500\n",
      " - 0s - loss: 0.6538 - accuracy: 0.6187 - val_loss: 0.6469 - val_accuracy: 0.6000\n",
      "Epoch 344/500\n",
      " - 0s - loss: 0.6411 - accuracy: 0.6467 - val_loss: 0.6467 - val_accuracy: 0.6000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 345/500\n",
      " - 0s - loss: 0.6506 - accuracy: 0.6206 - val_loss: 0.6453 - val_accuracy: 0.6000\n",
      "Epoch 346/500\n",
      " - 0s - loss: 0.6398 - accuracy: 0.6374 - val_loss: 0.6457 - val_accuracy: 0.6167\n",
      "Epoch 347/500\n",
      " - 0s - loss: 0.6518 - accuracy: 0.6411 - val_loss: 0.6453 - val_accuracy: 0.6000\n",
      "Epoch 348/500\n",
      " - 0s - loss: 0.6466 - accuracy: 0.6374 - val_loss: 0.6469 - val_accuracy: 0.6167\n",
      "Epoch 349/500\n",
      " - 0s - loss: 0.6500 - accuracy: 0.6224 - val_loss: 0.6466 - val_accuracy: 0.6167\n",
      "Epoch 350/500\n",
      " - 0s - loss: 0.6431 - accuracy: 0.6393 - val_loss: 0.6467 - val_accuracy: 0.6167\n",
      "Epoch 351/500\n",
      " - 0s - loss: 0.6411 - accuracy: 0.6374 - val_loss: 0.6491 - val_accuracy: 0.6167\n",
      "Epoch 352/500\n",
      " - 0s - loss: 0.6517 - accuracy: 0.6280 - val_loss: 0.6526 - val_accuracy: 0.6167\n",
      "Epoch 353/500\n",
      " - 0s - loss: 0.6440 - accuracy: 0.6318 - val_loss: 0.6508 - val_accuracy: 0.6167\n",
      "Epoch 354/500\n",
      " - 0s - loss: 0.6586 - accuracy: 0.6374 - val_loss: 0.6473 - val_accuracy: 0.6167\n",
      "Epoch 355/500\n",
      " - 0s - loss: 0.6482 - accuracy: 0.6168 - val_loss: 0.6500 - val_accuracy: 0.6167\n",
      "Epoch 356/500\n",
      " - 0s - loss: 0.6484 - accuracy: 0.6168 - val_loss: 0.6502 - val_accuracy: 0.6167\n",
      "Epoch 357/500\n",
      " - 0s - loss: 0.6402 - accuracy: 0.6393 - val_loss: 0.6484 - val_accuracy: 0.6167\n",
      "Epoch 358/500\n",
      " - 0s - loss: 0.6462 - accuracy: 0.6299 - val_loss: 0.6470 - val_accuracy: 0.6167\n",
      "Epoch 359/500\n",
      " - 0s - loss: 0.6442 - accuracy: 0.6299 - val_loss: 0.6450 - val_accuracy: 0.6167\n",
      "Epoch 360/500\n",
      " - 0s - loss: 0.6423 - accuracy: 0.6299 - val_loss: 0.6446 - val_accuracy: 0.6000\n",
      "Epoch 361/500\n",
      " - 0s - loss: 0.6451 - accuracy: 0.6336 - val_loss: 0.6438 - val_accuracy: 0.6333\n",
      "Epoch 362/500\n",
      " - 0s - loss: 0.6444 - accuracy: 0.6318 - val_loss: 0.6444 - val_accuracy: 0.6333\n",
      "Epoch 363/500\n",
      " - 0s - loss: 0.6366 - accuracy: 0.6355 - val_loss: 0.6458 - val_accuracy: 0.6167\n",
      "Epoch 364/500\n",
      " - 0s - loss: 0.6492 - accuracy: 0.6318 - val_loss: 0.6481 - val_accuracy: 0.6167\n",
      "Epoch 365/500\n",
      " - 0s - loss: 0.6455 - accuracy: 0.6187 - val_loss: 0.6475 - val_accuracy: 0.6167\n",
      "Epoch 366/500\n",
      " - 0s - loss: 0.6450 - accuracy: 0.6336 - val_loss: 0.6474 - val_accuracy: 0.6000\n",
      "Epoch 367/500\n",
      " - 0s - loss: 0.6498 - accuracy: 0.6393 - val_loss: 0.6506 - val_accuracy: 0.6167\n",
      "Epoch 368/500\n",
      " - 0s - loss: 0.6461 - accuracy: 0.6393 - val_loss: 0.6561 - val_accuracy: 0.6167\n",
      "Epoch 369/500\n",
      " - 0s - loss: 0.6475 - accuracy: 0.6336 - val_loss: 0.6582 - val_accuracy: 0.6167\n",
      "Epoch 370/500\n",
      " - 0s - loss: 0.6535 - accuracy: 0.6336 - val_loss: 0.6515 - val_accuracy: 0.6167\n",
      "Epoch 371/500\n",
      " - 0s - loss: 0.6423 - accuracy: 0.6449 - val_loss: 0.6517 - val_accuracy: 0.5833\n",
      "Epoch 372/500\n",
      " - 0s - loss: 0.6362 - accuracy: 0.6505 - val_loss: 0.6552 - val_accuracy: 0.6167\n",
      "Epoch 373/500\n",
      " - 0s - loss: 0.6376 - accuracy: 0.6374 - val_loss: 0.6592 - val_accuracy: 0.6167\n",
      "Epoch 374/500\n",
      " - 0s - loss: 0.6433 - accuracy: 0.6355 - val_loss: 0.6585 - val_accuracy: 0.6167\n",
      "Epoch 375/500\n",
      " - 0s - loss: 0.6489 - accuracy: 0.6336 - val_loss: 0.6549 - val_accuracy: 0.6000\n",
      "Epoch 376/500\n",
      " - 0s - loss: 0.6423 - accuracy: 0.6299 - val_loss: 0.6540 - val_accuracy: 0.6167\n",
      "Epoch 377/500\n",
      " - 0s - loss: 0.6428 - accuracy: 0.6393 - val_loss: 0.6530 - val_accuracy: 0.6167\n",
      "Epoch 378/500\n",
      " - 0s - loss: 0.6484 - accuracy: 0.6243 - val_loss: 0.6530 - val_accuracy: 0.6167\n",
      "Epoch 379/500\n",
      " - 0s - loss: 0.6355 - accuracy: 0.6393 - val_loss: 0.6552 - val_accuracy: 0.6333\n",
      "Epoch 380/500\n",
      " - 0s - loss: 0.6375 - accuracy: 0.6262 - val_loss: 0.6537 - val_accuracy: 0.6167\n",
      "Epoch 381/500\n",
      " - 0s - loss: 0.6424 - accuracy: 0.6430 - val_loss: 0.6554 - val_accuracy: 0.6167\n",
      "Epoch 382/500\n",
      " - 0s - loss: 0.6506 - accuracy: 0.6224 - val_loss: 0.6577 - val_accuracy: 0.6333\n",
      "Epoch 383/500\n",
      " - 0s - loss: 0.6514 - accuracy: 0.6411 - val_loss: 0.6601 - val_accuracy: 0.6167\n",
      "Epoch 384/500\n",
      " - 0s - loss: 0.6384 - accuracy: 0.6561 - val_loss: 0.6598 - val_accuracy: 0.6000\n",
      "Epoch 385/500\n",
      " - 0s - loss: 0.6368 - accuracy: 0.6355 - val_loss: 0.6564 - val_accuracy: 0.6000\n",
      "Epoch 386/500\n",
      " - 0s - loss: 0.6375 - accuracy: 0.6187 - val_loss: 0.6537 - val_accuracy: 0.5833\n",
      "Epoch 387/500\n",
      " - 0s - loss: 0.6437 - accuracy: 0.6336 - val_loss: 0.6537 - val_accuracy: 0.6000\n",
      "Epoch 388/500\n",
      " - 0s - loss: 0.6282 - accuracy: 0.6411 - val_loss: 0.6528 - val_accuracy: 0.5833\n",
      "Epoch 389/500\n",
      " - 0s - loss: 0.6409 - accuracy: 0.6505 - val_loss: 0.6602 - val_accuracy: 0.5833\n",
      "Epoch 390/500\n",
      " - 0s - loss: 0.6400 - accuracy: 0.6262 - val_loss: 0.6639 - val_accuracy: 0.6167\n",
      "Epoch 391/500\n",
      " - 0s - loss: 0.6376 - accuracy: 0.6336 - val_loss: 0.6581 - val_accuracy: 0.6167\n",
      "Epoch 392/500\n",
      " - 0s - loss: 0.6473 - accuracy: 0.6187 - val_loss: 0.6514 - val_accuracy: 0.6333\n",
      "Epoch 393/500\n",
      " - 0s - loss: 0.6416 - accuracy: 0.6467 - val_loss: 0.6613 - val_accuracy: 0.5833\n",
      "Epoch 394/500\n",
      " - 0s - loss: 0.6411 - accuracy: 0.6542 - val_loss: 0.6572 - val_accuracy: 0.6000\n",
      "Epoch 00394: early stopping\n",
      "roc_auc in train: 0.6836459137040773\n",
      "roc_auc in test: 0.6051703877790834\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lag = 10\n",
    "X_lstm = price_700_df_merged.drop([col for col in price_700_df_merged if '(t-' in col]+[\n",
    "    'Open(t+1) >= Close', 'Open(t+1)', 'date'], axis=1).copy()\n",
    "input_dim, input_col = X_lstm.shape[0], X_lstm.shape[1]\n",
    "\n",
    "minmax = MinMaxScaler(feature_range=(0, 1))\n",
    "X_lstm = minmax.fit_transform(X_lstm)\n",
    "#scaler = StandardScaler()\n",
    "#X_lstm = scaler.fit_transform(X_lstm)\n",
    " \n",
    "X_lstm_list, y_lstm_list = [], []\n",
    "for i in range(lag, len(X_lstm)):\n",
    "    X_lstm_list.append(X_lstm[i-lag:i, :])\n",
    "    y_lstm_list.append(np.array(price_700_df_merged['Open(t+1) >= Close'].iloc[i]))\n",
    "\n",
    "X_lstm_train, X_lstm_test, y_lstm_train, y_lstm_test = train_test_split(np.array(X_lstm_list)\n",
    "    , np.array(y_lstm_list), test_size=.1, random_state=3, stratify=np.array(y_lstm_list))\n",
    "print('X lstm train shape', X_lstm_train.shape)\n",
    "print('y lstm train shape', y_lstm_train.shape)\n",
    "\n",
    "#lstm model\n",
    "lstm_model = Sequential()\n",
    "# lstm_model.add(LSTM(units=64, dropout=.5, recurrent_dropout=.5, return_sequences=True))\n",
    "# lstm_model.add(Dropout(0.2))\n",
    "lstm_model.add(LSTM(units=10, dropout=.5, recurrent_dropout=.5))\n",
    "# lstm_model.add(BatchNormalization())\n",
    "# lstm_model.add(Dense(10, activation='elu'))\n",
    "# lstm_model.add(Dropout(0.2))\n",
    "lstm_model.add(Dense(units=1, activation='sigmoid'))\n",
    "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=100, verbose=2)\n",
    "history = lstm_model.fit(X_lstm_train, y_lstm_train, epochs=500, verbose=2, \n",
    "                         validation_data=[X_lstm_test, y_lstm_test], callbacks=[early_stopping], batch_size=32)\n",
    "\n",
    "print('roc_auc in train:', roc_auc_score(y_lstm_train, lstm_model.predict(X_lstm_train)))\n",
    "print('roc_auc in test:', roc_auc_score(y_lstm_test, lstm_model.predict(X_lstm_test)))\n",
    "\n",
    "# plot history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1,\n",
       "                   n_estimators=100, random_state=0)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clf = AdaBoostClassifier(n_estimators=100, random_state=0, learning_rate=1)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17,  7],\n",
       "       [12, 25]])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6885245901639344"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         col  importance\n",
      "75        sp500_index_change        0.06\n",
      "38             trend_adx_neg        0.04\n",
      "84            bert_pca8_mean        0.03\n",
      "74       nasdaq_index_change        0.03\n",
      "268  sp500_index_change(t-2)        0.03\n",
      "..                       ...         ...\n",
      "36                 trend_adx        0.01\n",
      "249          trend_psar(t-2)        0.01\n",
      "198           volume_em(t-2)        0.01\n",
      "323      trend_ema_fast(t-3)        0.00\n",
      "327       trend_adx_neg(t-3)        0.00\n",
      "\n",
      "[75 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "importance_feature_df = pd.DataFrame({'col': X_train.columns, \n",
    "                                      'importance': clf.feature_importances_})\n",
    "\n",
    "print(importance_feature_df.sort_values('importance', ascending=False).head(75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc in train: 0.999971275923479\n",
      "roc_auc in test: 0.6587837837837838\n"
     ]
    }
   ],
   "source": [
    "print('roc_auc in train:', roc_auc_score(y_train, clf.predict_proba(X_train)[:,  1]))\n",
    "print('roc_auc in test:', roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agni-ubuntu/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "    kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "    shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.SVC()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6065573770491803"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
