{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert reference link\n",
    "# https://medium.com/@aieeshashafique/feature-extraction-from-bert-25887ed2152a\n",
    "# https://github.com/AyeshaShafique/bert-feature-extraction-tf-2.0/blob/master/bert_embeddings_with_tensorflow_2_0.ipynb\n",
    "# https://colab.research.google.com/drive/1hMLd5-r82FrnFnBub-B-fVW78Px4KPX1#scrollTo=Ik3xqHqXM_lN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install tensorflow==2.0\n",
    "#!pip install tensorflow_hub #0.8.0\n",
    "#!pip install bert-for-tf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model \n",
    "import bert\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version:  2.0.0\n",
      "Hub version:  0.8.0\n"
     ]
    }
   ],
   "source": [
    "print(\"TF version: \", tf.__version__)\n",
    "print(\"Hub version: \", hub.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 256\n",
    "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    " name=\"input_word_ids\")\n",
    "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    " name=\"input_mask\")\n",
    "segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    " name=\"segment_ids\")\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    " trainable=False)\n",
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=[pooled_output, sequence_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See BERT paper: https://arxiv.org/pdf/1810.04805.pdf\n",
    "# And BERT implementation convert_single_example() at https://github.com/google-research/bert/blob/master/run_classifier.py\n",
    "\n",
    "def get_masks(tokens, max_seq_length):\n",
    "    \"\"\"Mask for padding\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def get_segments(tokens, max_seq_length):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    segments = []\n",
    "    current_segment_id = 0\n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            current_segment_id = 1\n",
    "    return segments + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def get_ids(tokens, tokenizer, max_seq_length):\n",
    "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "FullTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "#https://github.com/google-research/bert/blob/master/tokenization.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source file\n",
    "data_string = '20171001-20200430'\n",
    "path_news = './data/news_{}.gzip'.format(data_string)\n",
    "path_price = './data/price_{}.gzip'.format(data_string)\n",
    "df_news = pd.read_csv(path_news,compression='gzip',index_col = 0)\n",
    "df_price = pd.read_csv(path_price,compression='gzip',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pastnews = pd.read_csv('./bert_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234\n"
     ]
    }
   ],
   "source": [
    "# check the max len of the whole thing\n",
    "checklen = []\n",
    "for i in np.arange(df_news.shape[0]):\n",
    "\n",
    "    s1 = df_news.loc[i,'title']\n",
    "    s2 = df_news.loc[i,'description']\n",
    "    if type(s1)!= str: \n",
    "        s1=\"\"\n",
    "    if type(s2)!= str: \n",
    "        s2=\"\"\n",
    "            \n",
    "    \n",
    "    stokens1 = tokenizer.tokenize(s1)\n",
    "    stokens2 = tokenizer.tokenize(s2)\n",
    "    stokens = [\"[CLS]\"] + stokens1 + [\"[SEP]\"]+ stokens2 + [\"[SEP]\"]\n",
    "    checklen.append(len(stokens))\n",
    "    \n",
    "print(np.max(checklen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bert_output = pd.DataFrame(columns = ['title']+['output_'+str(i) for i in np.arange(768)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s = \"Hi we are using BERT\"\n",
    "#s = \"I'm testing the model with adfadfasdfafd wrong words\"\n",
    "\n",
    "for i in np.arange(df_news.shape[0]):\n",
    "\n",
    "    s1 = df_news.loc[i,'title']\n",
    "    s2 = df_news.loc[i,'description']\n",
    "    if type(s1)!= str: \n",
    "        s1=\"\"\n",
    "    if type(s2)!= str: \n",
    "        s2=\"\"\n",
    "            \n",
    "    stokens1 = tokenizer.tokenize(s1)\n",
    "    stokens2 = tokenizer.tokenize(s2)\n",
    "    stokens = [\"[CLS]\"] + stokens1 + [\"[SEP]\"]+ stokens2 + [\"[SEP]\"]\n",
    "    \n",
    "    input_ids = get_ids(stokens, tokenizer, max_seq_length)\n",
    "    input_masks = get_masks(stokens, max_seq_length)\n",
    "    input_segments = get_segments(stokens, max_seq_length)\n",
    "    pool_embs, all_embs = model.predict([[input_ids],[input_masks],[input_segments]])\n",
    "    df_bert_output.loc[i,'title'] = s1 \n",
    "    df_bert_output.loc[i, 1:] = pool_embs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bert_output.to_csv('bert_output{}.gzip'.format(data_string),index=False,compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bert_output.to_csv('bert_output{}.csv'.format(data_string),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5347 out of 5348"
     ]
    }
   ],
   "source": [
    "#s = \"Hi we are using BERT\"\n",
    "#s = \"I'm testing the model with adfadfasdfafd wrong words\"\n",
    "\n",
    "for i in np.arange(df_news.shape[0]):\n",
    "    print(\"\\r {} out of {}\".format(i,len(df_news)),end=\"\")\n",
    "    s1 = df_news.loc[i,'title']\n",
    "    s2 = df_news.loc[i,'description']\n",
    "    if type(s1)!= str: \n",
    "        s1=\"\"\n",
    "    if type(s2)!= str: \n",
    "        s2=\"\"\n",
    "            \n",
    "    if ((df_pastnews['title']==s1).any()==True) and s1!= \"\":\n",
    "        df_bert_output.loc[i,'title'] = s1 \n",
    "        df_bert_output.loc[i, 1:] = df_pastnews[df_pastnews['title']==s1].values[0][1:]\n",
    "    else:\n",
    "        stokens1 = tokenizer.tokenize(s1)\n",
    "        stokens2 = tokenizer.tokenize(s2)\n",
    "        stokens = [\"[CLS]\"] + stokens1 + [\"[SEP]\"]+ stokens2 + [\"[SEP]\"]\n",
    "\n",
    "        input_ids = get_ids(stokens, tokenizer, max_seq_length)\n",
    "        input_masks = get_masks(stokens, max_seq_length)\n",
    "        input_segments = get_segments(stokens, max_seq_length)\n",
    "        pool_embs, all_embs = model.predict([[input_ids],[input_masks],[input_segments]])\n",
    "        df_bert_output.loc[i,'title'] = s1 \n",
    "        df_bert_output.loc[i, 1:] = pool_embs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for inspecting individual record -- testing\n",
    "_rows = 1432\n",
    "s1 = df_news.loc[_rows,'title']\n",
    "s2 = df_news.loc[_rows,'description']\n",
    "stokens1 = tokenizer.tokenize(s1)\n",
    "stokens2 = tokenizer.tokenize(s2)\n",
    "stokens = [\"[CLS]\"] + stokens1 + [\"[SEP]\"]+ stokens2 + [\"[SEP]\"]\n",
    "input_ids = get_ids(stokens, tokenizer, max_seq_length)\n",
    "input_masks = get_masks(stokens, max_seq_length)\n",
    "input_segments = get_segments(stokens, max_seq_length)\n",
    "pool_embs, all_embs = model.predict([[input_ids],[input_masks],[input_segments]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>content</th>\n",
       "      <th>from</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1432</th>\n",
       "      <td>Lulu Yilun Chen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A Chinese e-commerce site called Pinduoduo, or...</td>\n",
       "      <td>https://www.bloomberg.com/news/articles/2018-0...</td>\n",
       "      <td>2018-06-30T06:08:00Z</td>\n",
       "      <td>A Chinese e-commerce site called Pinduoduo, or...</td>\n",
       "      <td>Bloomberg</td>\n",
       "      <td>2018-06-30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               author title  \\\n",
       "1432  Lulu Yilun Chen   NaN   \n",
       "\n",
       "                                            description  \\\n",
       "1432  A Chinese e-commerce site called Pinduoduo, or...   \n",
       "\n",
       "                                                    url           publishedAt  \\\n",
       "1432  https://www.bloomberg.com/news/articles/2018-0...  2018-06-30T06:08:00Z   \n",
       "\n",
       "                                                content       from        date  \n",
       "1432  A Chinese e-commerce site called Pinduoduo, or...  Bloomberg  2018-06-30  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news[pd.isnull(df_news['title'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'ten', '##cent', 'music', 'explores', 'anticipated', 'huge', 'ip', '##o', 'for', 'its', 'streaming', 'music', 'service', '[SEP]', 'china', '’', 's', 'largest', 'music', 'streaming', 'company', ',', 'ten', '##cent', 'music', 'entertainment', 'group', ',', 'is', 'poised', 'to', 'create', 'an', 'initial', 'public', 'offering', '(', 'ip', '##o', ')', 'and', 'is', 'negotiating', 'with', 'several', 'banks', 'for', 'under', '##writing', '.', 'the', 'wall', 'street', 'journal', 'reported', 'that', 'the', 'successful', 'debut', 'of', 'spot', '##ify', 't', '…', '[SEP]']\n",
      "[101, 2702, 13013, 2189, 15102, 11436, 4121, 12997, 2080, 2005, 2049, 11058, 2189, 2326, 102, 2859, 1521, 1055, 2922, 2189, 11058, 2194, 1010, 2702, 13013, 2189, 4024, 2177, 1010, 2003, 22303, 2000, 3443, 2019, 3988, 2270, 5378, 1006, 12997, 2080, 1007, 1998, 2003, 18875, 2007, 2195, 5085, 2005, 2104, 18560, 1012, 1996, 2813, 2395, 3485, 2988, 2008, 1996, 3144, 2834, 1997, 3962, 8757, 1056, 1529, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(stokens)\n",
    "print(input_ids)\n",
    "print(input_masks)\n",
    "print(input_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
